<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Global site tag (gtag.js) - Google Analytics -->
<!--  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-175789805-1"></script>-->
  <script type="text/javascript" src="hidebib.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.10/dist/clipboard.min.js"></script>
  <!-- Academicons -->
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css" />
  <!-- TO HAVE ICONS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- Style -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href="style.css" rel="stylesheet" type="text/css" />
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-175789805-1');
  </script>

  <title>Sacha Morin</title>

  <meta name="author" content="Sacha Morin">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords"
    content="Sacha, Morin, Morin, Robotics, Robot, Deep, Learning, Self-Driving, Vehicles, Mobile, Sensor, Fusion, Manifold, Geometric, Self-Supervised Learning, Unsupervised">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü§ñ</text></svg>">
  <meta property="og:type" content="website"/>
</head>

<body>

  <!-- Banner -->
  <table
    style="width:100%;max-width:900px;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding: 0;">
        <td style="padding:0">

          <div class="containersmall">
            <table
              style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tr style="padding:0">
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="index.html" target="_self" style="font-size:22px">About me</a>
                </td>
                  <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                      <a href="#featured" target="_self" style="font-size:22px">Featured</a>
                  </td>
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="#news" target="_self" style="font-size:22px">News</a>
                </td>
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="#pub_sec" target="_self" style="font-size:22px">Publications</a>
                </td>
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="#proj_sec" target="_self" style="font-size:22px">Projects</a>
                </td>
              </tr>
            </table>
          </div>
          <br>

          <!-- Intro -->
          <div class="containersmall">
            <table
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0">
                  <td
                    style="padding-top:1%;padding-bottom:1%;padding-left:4%;padding-right:4%;width:67%;vertical-align:middle">
                    <p style="text-align:center">
                      <name>Sacha Morin</name>
                    </p>
                    <p>
                      I am a PhD student in Machine Learning at <a
                        href="https://www.umontreal.ca/">Universit√© de Montr√©al</a> and <a
                        href="https://mila.quebec/en/">MILA</a>. I am advised by
                      <a href="https://www.guywolf.org">Guy Wolf</a> in the <a href="https://www.diffusion.space/team/team.html">RAFALES</a> lab
                      and <a href="http://liampaull.ca/">Liam Paull</a> in the
                      <a href="https://montrealrobotics.ca/people.html">Robotics and Embodied AI Lab (REAL)</a> lab.
                    </p>
                    <p>I obtained a BSc in Mathematics and Computer Science from the Universit√© de Montr√©al in 2021 and a Bachelor of Law from
                    the <a href="https://www.usherbrooke.ca/">Universit√© de Sherbrooke</a> in 2017.</p>
                    <p style="text-align:center">
                      <a href="mailto:sacha.morin@mila.quebec"><i style="font-size:16px"
                          class="fa fa-envelope"></i> &nbsp sacha.morin@mila.quebec &nbsp <i style="font-size:16px" class="fa fa-envelope"></i></a>
                    </p>
                    <hr>
                    <!-- Icons -->
                    <p style="text-align:center">
                      <a href="https://github.com/sachaMorin"><span style="font-size:24px"
                          class="social-icon fa fa-github"></span></a> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp
                      <!-- <a href=""><span  class="social-icon fa fa-skype"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  -->
                      <a href="data/sachamorin_cv.pdf" style="font-size:24px" class="links"> CV </a> &nbsp &nbsp
                      &nbsp &nbsp &nbsp &nbsp
                      <a href="https://scholar.google.com/citations?user=mIMinkMAAAAJ&hl=en&oi=ao"><span
                          style="font-size:24px" class="ai ai-google-scholar ai"></span></a> &nbsp &nbsp &nbsp &nbsp
                      &nbsp &nbsp
                      <a href="https://www.linkedin.com/in/sacha-morin-117752124/?locale=en_US"><span style="font-size:24px"
                          class="social-icon fa fa-linkedin"></span></a> &nbsp  &nbsp  &nbsp  &nbsp
                      <a href="https://twitter.com/SachMorin"><span style="font-size:24px" class="social-icon fa fa-twitter"></span></a>
                    </p>
                    <hr>
                  </td>
                  <td style="padding:2.5%;width:33%;max-width:33%">
                    <a href="https://www.linkedin.com/in/sacha-morin-117752124/?locale=en_US"><img style="width:100%;max-width:100%"
                        alt="profile photo" src="images/sacha.png" class="hoverZoomLink"></a>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <br>

          <!-- Research -->
          <div class="containersmall">
            <table id="research_sec"
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td
                    style="padding-top:1%;padding-bottom:1%;padding-left:4%;padding-right:4%;width:100%;vertical-align:middle;">
                    <p style="text-align:center">
                      <heading>Research</heading>
                    </p>
                    <p>
                      My research interests include:
                    </p>
                      <ul>
                          <li>Visual navigation, topological navigation and mobile robotics.</li>
                          <li>Learning structured and interpretable representations with deep learning and manifold learning.</li>
                          <li>Foundation models for robotics.</li>
                      </ul>
                      <p>
                          My <a href="data/sachamorin_cv.pdf">CV</a> includes more details.
                      </p>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <br>

            <!-- Featured Work -->
            <div class="containersmall" id="featured">
                <table
                        style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <tr>
                        <td colspan="2" style="padding:1%;width:100%">
                            <p style="text-align:center">
                                <heading>Featured Work</heading>
                            </p>
                        </td>
                        <td></td>
                    </tr>

                    <!-- One4All -->
                    <tr onmouseout="o4a_real_stop()" onmouseover="o4a_real_start()">
                        <td
                                style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                            <div class="aligned">
                                <!--                      <div class="two" id='o4a_real_seg'>-->
                                <!--                        <img src='images/o4a_real_after.gif' width="160">-->
                                <!--                      </div>-->
                                <img src='images/o4a.gif' width="160">
                            </div>
                            <script type="text/javascript">
                                function o4a_real_start() {
                                    document.getElementById('o4a_real_seg').style.opacity = "1";
                                }

                                function o4a_real_stop() {
                                    document.getElementById('o4a_real_seg').style.opacity = "0";
                                }
                                o4a_real_stop()
                            </script>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <a href="https://montrealrobotics.ca/o4a/">
                                <papertitle>One-4-All: Neural Potential Fields for Embodied Navigation
                                </papertitle>
                            </a>
                            <br>
                            <strong>Sacha Morin &#42;</strong>,
                            <a href="https://mikes96.github.io/">Miguel Saavedra-Ruiz &#42;</a>,
                            <a href="https://liampaull.ca/">Liam Paull</a>
                            <br>
                            <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2023
                            <br>
                            <div class="paper" id="o4a_real_ft">
                                <a href="https://github.com/montrealrobotics/one4all">code</a> /
                                <a href="https://arxiv.org/abs/2303.04011">arXiv</a> /
                                <a href="https://montrealrobotics.ca/o4a/">webpage</a> /
                                <a href="javascript:togglebib('o4a_real_ft')" class="togglebib">bibtex</a>
                                <pre id="myid" xml:space="preserve">
@article{morin2023one,
	title        = {One-4-All: Neural Potential Fields for Embodied Navigation},
	author       = {Morin, Sacha and Saavedra-Ruiz, Miguel and Paull, Liam},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2303.04011}
}</pre>

                            </div>
                            <p></p>
                            <div class="paper" id="o4a_real_description_ft">
                                <h6><a href="javascript:togglebib('o4a_real_description_ft')" class="togglebib">An end-to-end fully
                                    parametric method for image-goal navigation that leverages self-supervised and manifold learning to replace a topological graph with a geodesic regressor. During navigation, the geodesic regressor is used as an attractor in a potential function defined in latent space, allowing to frame navigation as a minimization problem.</a></h6>
                                <pre xml:space="preserve">
A fundamental task in robotics is to navigate between two locations.
In particular, real-world navigation can require long-horizon planning
using high-dimensional RGB images, which poses a substantial challenge
for end-to-end learning-based approaches. Current semi-parametric methods
instead achieve long-horizon navigation by combining learned modules with
a topological memory of the environment, often represented as a graph over
previously collected images. However, using these graphs in practice typically
involves tuning a number of pruning heuristics to avoid spurious edges, limit
runtime memory usage and allow reasonably fast graph queries. In this work,
we present One-4-All (O4A), a method leveraging self-supervised and manifold
learning to obtain a graph-free, end-to-end navigation pipeline in which the
goal is specified as an image. Navigation is achieved by greedily minimizing
a potential function defined continuously over the O4A latent space. Our system
is trained offline on non-expert exploration sequences of RGB data and controls,
and does not require any depth or pose measurements. We show that O4A can reach
long-range goals in 8 simulated Gibson indoor environments, and further demonstrate
successful real-world navigation using a Jackal UGV platform.
                      </pre>
                            </div>
                        </td>
                    </tr>

                    <!-- Break of line-->
                    <tr>
                        <td colspan="2" style="padding:0; margin:0;">
                            <hr style="padding:0; margin:0;">
                        </td>
                    </tr>

                     <!-- ConceptGraphs -->
                <tr onmouseout="concept_graphs_stop()" onmouseover="concept_graphs_start()">
                    <td
                            style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                        <div class="aligned">
                            <!--                      <div class="two" id='o4a_real_seg'>-->
                            <!--                        <img src='images/o4a_real_after.gif' width="160">-->
                            <!--                      </div>-->
                            <img src='images/conceptgraphs.gif' width="160">
                        </div>
                        <script type="text/javascript">
                            function concept_graphs_start() {
                                document.getElementById('concept_graphs_seg').style.opacity = "1";
                            }

                            function concept_graphs_stop() {
                                document.getElementById('concept_graphs_seg').style.opacity = "0";
                            }
                            o4a_real_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://concept-graphs.github.io/">
                            <papertitle> ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning
                            </papertitle>
                        </a>
                        <br>
                        <a href="https://georgegu1997.github.io/">Qiao Gu &#42;</a>,
                        <a href="https://www.alihkw.com/">Alihusein Kuwajerwala &#42;</a>,
                        <strong>Sacha Morin &#42;</strong>,
                        <a href="https://krrish94.github.io">Krishna Murthy Jatavallabhula &#42;</a>,
                        <a href="https://bipashasen.github.io/">Bipasha Sen</a>,
                        <a href="https://skymanaditya1.github.io/">Aditya Agarwal</a>,
                        <a
                                href="https://www.jhuapl.edu/work/our-organization/research-and-exploratory-development/red-staff-directory/corban-rivera">Corban
                            Rivera</a>,
                        <a href="https://scholar.google.com/citations?user=92bmh84AAAAJ">William Paul</a>,
                        <a href="https://mila.quebec/en/person/kirsty-ellis/">Kirsty Ellis</a>,
                        <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>,
                        <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>,
                        <a href="https://celsodemelo.net/">Celso Miguel de Melo</a>,
                        <a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>,
                        <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>,
                        <a href="http://www.cs.toronto.edu/\~florian/">Florian Shkurti</a>,
                        <a href="http://liampaull.ca">Liam Paull</a>
                        <!--                        <br>-->
                        <!--                        <em>Where it got published. -->
                        <!--                        <br>-->
                        <div class="paper" id="concept_graphs_ft">
                            <a href="https://github.com/concept-graphs/concept-graphs">code</a> /
                            <a href="http://arxiv.org/abs/2309.16650">arXiv</a> /
                            <a href="https://concept-graphs.github.io/">webpage</a> /
                            <a href="javascript:togglebib('concept_graphs_bib_ft')" class="togglebib">bibtex</a>
                            <pre id="concept_graphs_bib_ft" xml:space="preserve">
@article{conceptgraphs,
  author    = {Gu, Qiao and Kuwajerwala, Alihusein and Morin, Sacha and Jatavallabhula, {Krishna Murthy} and  Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and Gan, Chuang and {de Melo}, {Celso Miguel} and Tenenbaum, {Joshua B.} and Torralba, Antonio and Shkurti, Florian and Paull, Liam},
  title     = {ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning},
  journal   = {arXiv},
  year      = {2023},
}
}</pre>

                        </div>
                        <p></p>
                        <div class="paper" id="concept_graphs_description_ft">
                            <h6><a href="javascript:togglebib('concept_graphs_description_ft')" class="togglebib">ConceptGraphs uses off-the-shelf models to build an object-based map from RGB-D images. Objects have associated multi-view fused CLIP features and language captions that can be leveraged by robots to answer abstract queries.</a></h6>
                            <pre xml:space="preserve">
For robots to perform a wide variety of tasks, they require a 3D representation
of the world that is semantically rich, yet compact and efficient for task-driven
perception and planning. Recent approaches have attempted to leverage features from
large vision-language models to encode semantics in 3D representations. However,
these approaches tend to produce maps with per-point feature vectors, which do not
scale well in larger environments, nor do they contain semantic spatial relationships
between entities in the environment, which are useful for downstream planning.
In this work, we propose ConceptGraphs, an open-vocabulary graph-structured
representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models
and fusing their output to 3D by multi-view association. The resulting representations
generalize to novel semantic classes, without the need to collect large 3D datasets or
finetune models. We demonstrate the utility of this representation through a number of
downstream planning tasks that are specified through abstract (language) prompts and
require complex reasoning over spatial and semantic concepts.
                      </pre>
                        </div>
                    </td>
                </tr>

                    </tbody>
                </table>
            </div>
            <br>

          <!-- News -->
          <div class="containersmall" id="news">
            <table
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td
                    style="padding-top:1%;padding-bottom:1%;padding-left:4%;padding-right:4%;width:100%;vertical-align:middle">
                    <p style="text-align:center">
                      <heading>News</heading>
                    </p>
                    <ul>
                        <li>
                            <strong>[July 2023]</strong> <a href="https://montrealrobotics.ca/o4a/">One-4-All</a> has been accepted to IROS 2023!
                        </li>
                     <li>
                         <strong>[July 2023]</strong> I was a participant in the  <a href="https://robotics-summerschool.ethz.ch/">ETH Robotics Summer School</a> in Geneva, Switzerland. We worked on an autonomous rough-terrain UGV for search and rescue operations.
                     </li>
                      <li>
                          <strong>[June 2023]</strong> I was one of the organizers of the  <a href="https://mila.quebec/en/robotics-summer-school-2023/">2023 Mila Robotics Summer School</a>.
                      </li>
                        <li>
                            <strong>[May 2023]</strong> I was awarded the <a href="https://www.nserc-crsng.gc.ca/students-etudiants/pg-cs/bellandpostgrad-belletsuperieures_eng.asp">NSERC PGS D Scholarship</a>!
                        </li>
                        <li>
                            <strong>[May 2023]</strong> I was awarded the <a href="https://frq.gouv.qc.ca/en/program/frqnt-doctoral-training-scholarships/">FRQNT Doctoral Scholarship</a>!
                        </li>
                      <li>
                        <strong>[January 2023]</strong> I will be TA for  <a href="https://admission.umontreal.ca/cours-et-horaires/cours/stt-3795/">STT3795 - Theoretical Foundations of Data Science</a>.
                      </li>
                      <li>
                        <strong>[December 2022]</strong> I gave a short talk on
                        <a href="https://litterature.uqam.ca/babillard/jeudi-15-decembre-conference-de-sacha-morin-donnees-algorithmes-ia-definitions-et-tour-dhorizon/">AI, Data & Algorithms</a> at UQ√ÄM in Sylvano Santini's <a href="https://etudier.uqam.ca/cours?sigle=SEM9500">SEM9500 Seminar</a>.
                      </li>
                      <li>
                        <strong>[November 2022]</strong> GRAE has been accepted in the <a href="https://ieeexplore.ieee.org/abstract/document/9950332">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</a>.
                      </li>
                      <li>
                        <strong>[September 2022]</strong> Happy to join the <a href="https://ivado.ca/en/student-committee/">IVADO
Student Intersectoral Committee</a> for 2022-2023!
                      </li>
                      <li>
                        <strong>[September 2022]</strong> Started a PhD in Machine Learning with Guy Wolf at
                        Universit√© de Montr√©al and MILA. Still working on combining manifold learning techniques and
                        deep learning!
                      </li>
                      <li>
                        <strong>[March 2022]</strong> Presented our project
                        <a href="https://arxiv.org/abs/2203.03682">Monocular Robot Navigation with Self-Supervised
                          Pretrained Vision Transformers</a> at the <a href="https://ivado.ca/en/events/my-ivado-project-in-180-seconds/">My IVADO project in 180 seconds</a> event.
                          Watch the presentation on <a href="https://www.youtube.com/watch?v=CYjd7WxgKOw">Youtube</a> [French].
                      </li>
                      <li>
                        <strong>[April 2022]</strong> Our project
                        <a href="https://arxiv.org/abs/2203.03682">Monocular Robot Navigation with Self-Supervised
                          Pretrained Vision Transformers</a> was featured on
                        <a href="https://www.duckietown.org/archives/88017"> Duckietown's webpage!</a>
                      </li>
                      <li>
                        <strong>[March 2022]</strong> Our paper
                          <a href="https://arxiv.org/abs/2203.03682">Monocular Robot Navigation with
                          Self-Supervised Pretrained Vision Transformers</a> has been accepted for Poster presentation
                          at the 19th <a href="https://www.computerrobotvision.org/#"> Conference on Robotics and Vision
                          (CRV).</a>
                      </li>
                      <li>
                        <strong>[October 2021]</strong> Presented an ongoing project on COVID-19 data at the <strong>IVADO Digital October</strong>. Watch the presentation on <a href="https://www.youtube.com/watch?v=L9e5fuOmaeI&list=PLnI094tgrXpxIQ_p9IGpb-ukUUNK6x9A2&index=19&t=22s">Youtube</a> [French].
                      </li>
                      <li>
                        <strong>[September 2021]</strong> Started a research MSc in Machine Learning with Guy Wolf at
                        Universit√© de Montr√©al and MILA. I will be working on blending manifold learning techniques and
                        deep learning.
                      </li>
                      <li>
                        <strong>[May 2021]</strong> I was awarded the <a href="https://ivado.ca/en/scholarships-and-grants/msc-excellence-scholarships/">IVADO MSc Excellence Scholarship</a>!
                      </li>
                      <li>
                      <strong>[May 2021]</strong> I was awarded the <a href="https://frq.gouv.qc.ca/en/program/scholarships-2nd-and-3rd-cycle-2022-2023/">FRQNT B1X Scholarship</a>!
                      </li>
                      <li>
                        <strong>[December 2020]</strong> <a href="https://arxiv.org/abs/2007.07142">GRAE</a> has been accepted to the <a href="https://ieeexplore.ieee.org/document/9378049">IEEE International Conference on Big Data</a>.
                      </li>
                      <li>
                        <strong>[December 2020]</strong> Presented <a href="https://arxiv.org/abs/2007.07142">GRAE</a> at the <a href="https://www.google.com/search?q=neurips&oq=neurips&aqs=chrome..69i57j69i59l3j0i271j69i60l3.672j0j7&sourceid=chrome&ie=UTF-8">DiffGeo4DL NeurIPS Workshop</a>. The presentation can be watched under the "Extendable and invertible manifold learning with geometry regularized autoencoders" section [English].
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <br>

          <!-- Publications -->
          <div class="containersmall" id="pub_sec">
            <table
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td colspan="2" style="padding:1%;width:100%">
                    <p style="text-align:center">
                      <heading>Publications</heading>
                    </p>
                  </td>
                  <td></td>
                </tr>

                <!-- One4All -->
                <tr onmouseout="o4a_real_stop()" onmouseover="o4a_real_start()">
                    <td
                            style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                        <div class="aligned">
                            <!--                      <div class="two" id='o4a_real_seg'>-->
                            <!--                        <img src='images/o4a_real_after.gif' width="160">-->
                            <!--                      </div>-->
                            <img src='images/o4a.gif' width="160">
                        </div>
                        <script type="text/javascript">
                            function o4a_real_start() {
                                document.getElementById('o4a_real_seg').style.opacity = "1";
                            }

                            function o4a_real_stop() {
                                document.getElementById('o4a_real_seg').style.opacity = "0";
                            }
                            o4a_real_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://montrealrobotics.ca/o4a/">
                            <papertitle>One-4-All: Neural Potential Fields for Embodied Navigation
                            </papertitle>
                        </a>
                        <br>
                        <strong>Sacha Morin &#42;</strong>,
                        <a href="https://mikes96.github.io/">Miguel Saavedra-Ruiz &#42;</a>,
                        <a href="https://liampaull.ca/">Liam Paull</a>
                        <br>
                        <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2023
                        <br>
                        <div class="paper" id="o4a_real">
                            <a href="https://github.com/montrealrobotics/one4all">code</a> /
                            <a href="https://arxiv.org/abs/2303.04011">arXiv</a> /
                            <a href="https://montrealrobotics.ca/o4a/">webpage</a> /
                            <a href="javascript:togglebib('o4a_real')" class="togglebib">bibtex</a>
                            <pre id="myid" xml:space="preserve">
@article{morin2023one,
	title        = {One-4-All: Neural Potential Fields for Embodied Navigation},
	author       = {Morin, Sacha and Saavedra-Ruiz, Miguel and Paull, Liam},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2303.04011}
}</pre>

                        </div>
                        <p></p>
                        <div class="paper" id="o4a_real_description">
                            <h6><a href="javascript:togglebib('o4a_real_description')" class="togglebib">An end-to-end fully
                                parametric method for image-goal navigation that leverages self-supervised and manifold learning to replace a topological graph with a geodesic regressor. During navigation, the geodesic regressor is used as an attractor in a potential function defined in latent space, allowing to frame navigation as a minimization problem.</a></h6>
                            <pre xml:space="preserve">
A fundamental task in robotics is to navigate between two locations.
In particular, real-world navigation can require long-horizon planning
using high-dimensional RGB images, which poses a substantial challenge
for end-to-end learning-based approaches. Current semi-parametric methods
instead achieve long-horizon navigation by combining learned modules with
a topological memory of the environment, often represented as a graph over
previously collected images. However, using these graphs in practice typically
involves tuning a number of pruning heuristics to avoid spurious edges, limit
runtime memory usage and allow reasonably fast graph queries. In this work,
we present One-4-All (O4A), a method leveraging self-supervised and manifold
learning to obtain a graph-free, end-to-end navigation pipeline in which the
goal is specified as an image. Navigation is achieved by greedily minimizing
a potential function defined continuously over the O4A latent space. Our system
is trained offline on non-expert exploration sequences of RGB data and controls,
and does not require any depth or pose measurements. We show that O4A can reach
long-range goals in 8 simulated Gibson indoor environments, and further demonstrate
successful real-world navigation using a Jackal UGV platform.
                      </pre>
                        </div>
                    </td>
                </tr>
                <!-- Break of line-->
                <tr>
                    <td colspan="2" style="padding:0; margin:0;">
                        <hr style="padding:0; margin:0;">
                    </td>
                </tr>

                <!-- GRAE-->
                <tr onmouseout="grae_real_stop()" onmouseover="grae_real_start()">
                  <td
                          style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="aligned">
<!--                      <div class="two" id='grae_real_seg'>-->
                        <!--                        <img src='images/grae_real_before.png' width="160">-->
<!--                      </div>-->
                      <img src='images/grae_real_after.gif' width="160">
                    </div>
                    <script type="text/javascript">
                      function grae_real_start() {
                        document.getElementById('grae_real_seg').style.opacity = "1";
                      }

                      function grae_real_stop() {
                        document.getElementById('grae_real_seg').style.opacity = "0";
                      }
                      grae_real_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/KevinMoonLab/GRAE">
                      <papertitle>Geometry Regularized Autoencoders (GRAE)
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://scholar.google.com/citations?user=aplY5_gAAAAJ&hl=es">Andres F. Duque &#42;</a>,
                    <strong>Sacha Morin &#42;</strong>,
                    <a href="https://www.guywolf.org/">Guy Wolf</a>,
                    <a href="https://sites.google.com/a/umich.edu/kevin-r-moon/home">Kevin R. Moon</a>
                    <br>
                    <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2022 <br>
                    <em>IEEE International Conference on Big Data</em>, 2020 <br>
                    <em>DiffGeo4DL</em>, NeurIPS 2020 Workshop
                    <br>
                    <div class="paper" id="grae_real">
                      <a href="https://github.com/KevinMoonLab/GRAE">code</a> /
                      <a href="https://ieeexplore.ieee.org/abstract/document/9950332">paper</a> /
                      <a href="https://github.com/KevinMoonLab/GRAE">webpage</a> /
                      <a href="javascript:togglebib('grae_real')" class="togglebib">bibtex</a>
                      <pre id="myid" xml:space="preserve">
@article{duque2022geometry,
  title={Geometry Regularized Autoencoders},
  author={Duque, Andres F and Morin, Sacha and Wolf, Guy and Moon, Kevin R},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE}
}                     </pre>
                    </div>
                    <p></p>
                    <div class="paper" id="grae_real_description">
                      <h6><a href="javascript:togglebib('grae_real_description')" class="togglebib">Manifold-based regularization for learning better representations in autoencoders.</a></h6>
                      <pre>
A fundamental task in data exploration is to extract simplified
low dimensional representations that capture intrinsic geometry
in data, especially for faithfully visualizing data in two or
three dimensions. Common approaches to this task use kernel methods
for manifold learning. However, these methods typically only provide an
embedding of fixed input data and cannot extend to new data points.
Autoencoders have also recently become popular for representation
learning. But while they naturally compute feature extractors that
are both extendable to new data and invertible (i.e., reconstructing
original features from latent representation), they have limited capabilities
to follow global intrinsic geometry compared to kernel-based manifold learning.
We present a new method for integrating both approaches by incorporating a
geometric regularization term in the bottleneck of the autoencoder. Our
regularization, based on the diffusion potential distances from the
recently-proposed PHATE visualization method, encourages the learned latent
representation to follow intrinsic data geometry, similar to manifold learning
algorithms, while still enabling faithful extension to new data and reconstruction
of data in the original feature space from latent coordinates. We compare our
approach with leading kernel methods and autoencoder models for manifold learning
to provide qualitative and quantitative evidence of our advantages in preserving
intrinsic structure, out of sample extension, and reconstruction. Our method is easily
implemented for big-data applications, whereas other methods are limited in this regard.
                      </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>


                <!-- DINO Real Project -->
                <tr onmouseout="dino_real_stop()" onmouseover="dino_real_start()">
                  <td
                      style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='dino_real_seg'>
                        <img src='images/dino_real_after.gif' width="160">
                      </div>
                      <img src='images/dino_real_before.jpg' width="160">
                    </div>
                    <script type="text/javascript">
                      function dino_real_start() {
                        document.getElementById('dino_real_seg').style.opacity = "1";
                      }

                      function dino_real_stop() {
                        document.getElementById('dino_real_seg').style.opacity = "0";
                      }
                      dino_real_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://sachamorin.github.io/dino/">
                      <papertitle>Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://mikes96.github.io/">Miguel Saavedra-Ruiz &#42;</a>,
                    <strong>Sacha Morin &#42;</strong>,
                    <a href="https://liampaull.ca/">Liam Paull</a>
                    <br>
                    <em>Conference on Robotics and Vision (CRV)</em>, 2022
                    <br>
                    <div class="paper" id="dino_real">
                      <a href="https://github.com/sachaMorin/dino">code (model)</a> /
                      <a href="https://github.com/MikeS96/object-detection/tree/daffy">code (servoing)</a> /
                      <a href="https://arxiv.org/abs/2203.03682">arXiv</a> /
                      <a href="https://sachamorin.github.io/dino/">webpage</a> /
                      <a href="https://www.duckietown.org/archives/88017">duckietown coverage</a> /
                      <a href="data/duckieFormerPoster.png">poster</a> /
                      <a href="https://www.youtube.com/watch?v=CYjd7WxgKOw">youtube [FR]</a>/
                      <a href="javascript:togglebib('dino_real')" class="togglebib">bibtex</a>
                      <pre id="myid" xml:space="preserve">
@article{saavedra2022monocular,
	title        = {Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers},
	author       = {Saavedra-Ruiz, Miguel and Morin, Sacha and Paull, Liam},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2203.03682}
}                     </pre>
                    </div>
                    <p></p>
                    <div class="paper" id="dino_real_description">
                      <h6><a href="javascript:togglebib('dino_real_description')" class="togglebib">Visual Servoing navigation using pre-trained Self-Supervised Vision Transformers.</a></h6>
                      <pre xml:space="preserve">
In this work, we consider the problem of learning a perception model for
monocular robot navigation using few annotated images. Using a Vision
Transformer (ViT) pretrained with a label-free self-supervised method, we
successfully train a coarse image segmentation model for the Duckietown
environment using 70 training images. Our model performs coarse image
segmentation at the 8x8  patch level, and the inference resolution can be
adjusted to balance prediction granularity and real-time perception constraints.
We study how best to adapt a ViT to our task and environment, and find that some
lightweight architectures can yield good single-image segmentations at a usable
frame rate, even on CPU. The resulting perception model is used as the backbone
for a simple yet robust visual servoing agent, which we deploy on a differential
drive mobile robot to perform two tasks: lane following and obstacle avoidance. </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>



                <!-- PLOS One-->
                <tr>
                  <td
                          style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="aligncenter">
                      <br>
                      <img src='images/plos_one.png' width="160">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle;">
                    <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0260714">
                      <papertitle>
                          Patient health records and whole viral genomes from an early SARS-CoV-2 outbreak in a Quebec hospital reveal features associated with favorable outcomes
                      </papertitle>
                    </a>
                    <br>
                    Par√© et al.
                    <br>
                    <em>PLOS One</em>, 2021
                    <br>
                    <div class="paper" id="plos_one">
                      <a href="https://journals.plos.org/plosone/article/authors?id=10.1371/journal.pone.0260714">paper</a>/
                      <a href="javascript:togglebib('plos_one')" class="togglebib">bibtex</a>
                      <pre id="plos" xml:space="preserve">
@article{pare2021patient,
  title={Patient health records and whole viral genomes from an early SARS-CoV-2 outbreak in a Quebec hospital reveal features associated with favorable outcomes},
  author={Par{\'e}, Bastien and Rozendaal, Marieke and Morin, Sacha and Kaufmann, L{\'e}a and Simpson, Shawn M and Poujol, Rapha{\"e}l and Mostefai, Fatima and Grenier, Jean-Christophe and Xing, Henry and Sanchez, Miguelle and others},
  journal={PloS one},
  volume={16},
  number={12},
  pages={e0260714},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA}
}                     </pre>
                    </div>
                    <p></p>
                    <div class="paper" id="plos_description">
                      <h6><a>Analysis of patient outcomes in a SARS-Cov-2 outbreak in a Quebec hospital.</a></h6>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>



              </tbody>
            </table>
          </div>
          <br>

          <!-- Projects -->
          <div class="containersmall" id="proj_sec">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <tr>
                  <td colspan="2" style="padding:2%;width:100%">
                    <p style="text-align:center">
                      <heading>Projects and Preprints</heading>
                    </p>
                  </td>
                  <td></td>
                </tr>

                <!-- ConceptGraphs -->
                <tr onmouseout="concept_graphs_stop()" onmouseover="concept_graphs_start()">
                    <td
                            style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                        <div class="aligned">
                            <!--                      <div class="two" id='o4a_real_seg'>-->
                            <!--                        <img src='images/o4a_real_after.gif' width="160">-->
                            <!--                      </div>-->
                            <img src='images/conceptgraphs.gif' width="160">
                        </div>
                        <script type="text/javascript">
                            function concept_graphs_start() {
                                document.getElementById('concept_graphs_seg').style.opacity = "1";
                            }

                            function concept_graphs_stop() {
                                document.getElementById('concept_graphs_seg').style.opacity = "0";
                            }
                            o4a_real_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://concept-graphs.github.io/">
                            <papertitle> ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning
                            </papertitle>
                        </a>
                        <br>
                        <a href="https://georgegu1997.github.io/">Qiao Gu &#42;</a>,
                        <a href="https://www.alihkw.com/">Alihusein Kuwajerwala &#42;</a>,
                        <strong>Sacha Morin &#42;</strong>,
                        <a href="https://krrish94.github.io">Krishna Murthy Jatavallabhula &#42;</a>,
                        <a href="https://bipashasen.github.io/">Bipasha Sen</a>,
                        <a href="https://skymanaditya1.github.io/">Aditya Agarwal</a>,
                        <a
                                href="https://www.jhuapl.edu/work/our-organization/research-and-exploratory-development/red-staff-directory/corban-rivera">Corban
                            Rivera</a>,
                        <a href="https://scholar.google.com/citations?user=92bmh84AAAAJ">William Paul</a>,
                        <a href="https://mila.quebec/en/person/kirsty-ellis/">Kirsty Ellis</a>,
                        <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>,
                        <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>,
                        <a href="https://celsodemelo.net/">Celso Miguel de Melo</a>,
                        <a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>,
                        <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>,
                        <a href="http://www.cs.toronto.edu/\~florian/">Florian Shkurti</a>,
                        <a href="http://liampaull.ca">Liam Paull</a>
                        <!--                        <br>-->
                        <!--                        <em>Where it got published. -->
                        <!--                        <br>-->
                        <div class="paper" id="concept_graphs">
                            <a href="https://github.com/concept-graphs/concept-graphs">code</a> /
                            <a href="http://arxiv.org/abs/2309.16650">arXiv</a> /
                            <a href="https://concept-graphs.github.io/">webpage</a> /
                            <a href="javascript:togglebib('concept_graphs_bib')" class="togglebib">bibtex</a>
                            <pre id="concept_graphs_bib" xml:space="preserve">
@article{conceptgraphs,
  author    = {Gu, Qiao and Kuwajerwala, Alihusein and Morin, Sacha and Jatavallabhula, {Krishna Murthy} and  Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and Gan, Chuang and {de Melo}, {Celso Miguel} and Tenenbaum, {Joshua B.} and Torralba, Antonio and Shkurti, Florian and Paull, Liam},
  title     = {ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning},
  journal   = {arXiv},
  year      = {2023},
}
}</pre>

                        </div>
                        <p></p>
                        <div class="paper" id="concept_graphs_description">
                            <h6><a href="javascript:togglebib('concept_graphs_description')" class="togglebib">ConceptGraphs uses off-the-shelf models to build an object-based map from RGB-D images. Objects have associated multi-view fused CLIP features and language captions that can be leveraged by robots to answer abstract queries.</a></h6>
                            <pre xml:space="preserve">
For robots to perform a wide variety of tasks, they require a 3D representation
of the world that is semantically rich, yet compact and efficient for task-driven
perception and planning. Recent approaches have attempted to leverage features from
large vision-language models to encode semantics in 3D representations. However,
these approaches tend to produce maps with per-point feature vectors, which do not
scale well in larger environments, nor do they contain semantic spatial relationships
between entities in the environment, which are useful for downstream planning.
In this work, we propose ConceptGraphs, an open-vocabulary graph-structured
representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models
and fusing their output to 3D by multi-view association. The resulting representations
generalize to novel semantic classes, without the need to collect large 3D datasets or
finetune models. We demonstrate the utility of this representation through a number of
downstream planning tasks that are specified through abstract (language) prompts and
require complex reasoning over spatial and semantic concepts.
                      </pre>
                        </div>
                    </td>
                </tr>

                <!-- Break of line-->
                <tr>
                    <td colspan="2" style="padding:0; margin:0;">
                        <hr style="padding:0; margin:0;">
                    </td>
                </tr>

                <!-- COVID PHATE Project -->
                <tr>
                    <td
                            style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/covid-phate.png' width="160">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle;padding-top:1px">
                        <a href="https://www.medrxiv.org/content/10.1101/2023.06.14.23290814v1">
                            <papertitle>Sustained IFN signaling is associated with delayed development of SARS-CoV-2-specific immunity</papertitle>
                        </a>
                        <br>
                        <a href="https://www.linkedin.com/in/elsa-brunet-ratnasingham-9588b5140/">Elsa Brunet-Ratnasingham&#42;</a>,
                        <strong>Sacha Morin &#42;</strong>,
                        <a href="https://scholar.google.com/citations?user=fP9qw0AAAAAJ&hl=en">Haley E. Randolph&#42;</a> et al.
                        <br>
                        <a href="https://www.medrxiv.org/content/10.1101/2023.06.14.23290814v1">medrXiv</a>
                        <p></p>
                        <div class="paper" id="style_description">
                            <h6><a href="javascript:togglebib('style_description')" class="togglebib">Integrated
                            analysis using k-means and manifold learning to uncover endotypes in a cohort of COVID-19 patients.</a></h6>
                            <!--                      <pre xml:space="preserve">-->
                            <!--                      Abstract-->
                            <!--create a seamlessly implementation of style-transfer with an intuitive GUI. </pre>-->
                        </div>
                    </td>
                </tr>
                <!-- Break of line-->
                <tr>
                    <td colspan="2" style="padding:0; margin:0;">
                        <hr style="padding:0; margin:0;">
                    </td>
                </tr>

                <!-- StepMix -->
                <tr>
                  <td
                    style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <img src='images/stepmix.png' width="160">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2304.03853">
                      <papertitle>StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables</papertitle>
                    </a>
                    <br>
                    <strong>Sacha Morin &#42;</strong>,
                    <a href="https://www.linkedin.com/in/robin-legault-28b670139/">Robin Legault &#42;</a>,
                      <a>F√©lix Lalibert√©</a>,
                    <a href="https://www.universiteitleiden.nl/en/staffmembers/zsuzsa-bakk#tab-1">  Zsuzsa Bakk </a>,
                     <a href="https://www.linkedin.com/in/cegiguere/?originalSubdomain=ca"> Charles-√âdouard Gigu√®re </a>,
                     <a href="https://psy.umontreal.ca/repertoire-departement/professeures/professeures/in/in15244/sg/Roxane%20de%20la%20Sablonni%C3%A8re/"> Roxane de la Sablonni√®re </a>,
                    <a href="https://recherche.umontreal.ca/nos-chercheurs/repertoire-des-professeurs/chercheur/is/in14715/">√âric Lacourse</a>
                    <br>
                    <a href="https://github.com/Labo-Lacourse/stepmix">code</a> /
                    <a href="https://arxiv.org/abs/2304.03853">arXiv</a>
                    <p></p>
                    <div class="paper" id="style_description">
                      <h6><a href="javascript:togglebib('style_description')" class="togglebib">A Python package for
                        multi-step estimation of latent class models with measurement and structural components. Enables joint clustering
                      of continuous and categorical features with missing values.</a></h6>
<!--                      <pre xml:space="preserve">-->
<!--                      Abstract-->
<!--create a seamlessly implementation of style-transfer with an intuitive GUI. </pre>-->
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                    <td colspan="2" style="padding:0; margin:0;">
                        <hr style="padding:0; margin:0;">
                    </td>
                </tr>
                  <!-- COVID-19 Taskforce -->
                <tr>
                  <td
                          style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="aligncenter">
                      <img src='images/covidtaskforce.png' width="160">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://covid-taskforce.github.io/">
                      <papertitle>MILA COVID-19 Taskforce</papertitle>
                    </a>
                    <p></p>
                    <div class="paper" id="style_description">
                      <h6>
                               The <b>Mila COVID-19 Taskforce</b> is a collaboration between researchers to answer COVID-19 research questions
        via data-driven methods. Our team is composed of members from the Universit√© de Montr√©al, Yale University and
        McGill University. Participating research laboratories include <a href="https://mila.quebec/en/">Mila</a>,
        <a href="https://www.krishnaswamylab.org/">Krishnaswamy Lab</a>, <a href="https://mhi-omics.org/">MHI-omics</a>,
        <a href="https://microbiologie.umontreal.ca/en/professeurs-chercheurs/daniel-kaufmann/">Kaufmann Lab</a>,
        <a href="https://morgancraiglab.com/about">Quantitative and Translational Medicine Laboratory</a>, and
        <a href="https://www.therealsmithlab.com/">Smith Lab</a>.
                      </h6>
                      <!--                      <pre xml:space="preserve">-->
                      <!--                      Abstract-->
                      <!--create a seamlessly implementation of style-transfer with an intuitive GUI. </pre>-->
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <br>

<!--                &lt;!&ndash; Calendula Flower Experiments &ndash;&gt;-->
<!--                <tr onmouseout="calenF_stop()" onmouseover="calenF_start()">-->
<!--                  <td-->
<!--                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">-->
<!--                    <div class="one">-->
<!--                      <div class="two" id='calenF_image'>-->
<!--                        <img src='images/calendula_after.jpg' width="160">-->
<!--                      </div>-->
<!--                      <img src='images/calendula_after.jpg' width="160">-->
<!--                    </div>-->
<!--                    <script type="text/javascript">-->
<!--                      function calenF_start() {-->
<!--                        document.getElementById('calenF_image').style.opacity = "1";-->
<!--                      }-->

<!--                      function calenF_stop() {-->
<!--                        document.getElementById('calenF_image').style.opacity = "0";-->
<!--                      }-->
<!--                      calenF_stop()-->
<!--                    </script>-->
<!--                  </td>-->
<!--                  <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--                    <a href="data/calendulaFlower.txt">-->
<!--                      <papertitle>‚ÄãCalendula Flower Classification System</papertitle>-->
<!--                    </a>-->
<!--                    <br>-->
<!--                    <strong>Miguel Saavedra-Ruiz</strong>-->
<!--                    <br>-->
<!--                    <p></p>-->

<!--                    <div class="paper" id="calenF_description">-->
<!--                      <h6><a href="javascript:togglebib('calenF_description')" class="togglebib">Implementation of a-->
<!--                          low-cost Calendula flower classification system using CNN.</a></h6>-->
<!--                      <pre xml:space="preserve">-->
<!--Implemented a low cost Calendula flower classification system with Tensorflow, Android Studio and-->
<!--Arduino. The system used a deep convolutional neural network inception v3 trained with different-->
<!--Calendula flowers. When the system detected a Calendula flower, it sent a signal to an Arduino-->
<!--board and moved a servomotor to a specific point which allowed the correct classification of the-->
<!--flowers through a transportation band (Implemented in C, Python/Linux). </pre>-->
<!--                    </div>-->
<!--                  </td>-->
<!--                </tr>-->



          <!-- Certificates -->
<!--          <div class="containersmall" id="others">-->
<!--            <table-->
<!--              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
<!--              <tbody>-->
<!--                <tr>-->
<!--                  <td-->
<!--                    style="padding-top:1%;padding-bottom:1%;padding-left:4%;padding-right:4%;width:100%;vertical-align:middle">-->
<!--                    <p style="text-align:center" id="cert_sec">-->
<!--                      <heading>Courses and Certifications</heading>-->
<!--                    </p>-->
<!--                    <ul>-->
<!--                      <li>-->
<!--                        <strong>Reinforcement Learning</strong> by University of Alberta & Alberta Machine Intelligence-->
<!--                        Institute on Coursera. Certificate earned at June 21, 2020. <a-->
<!--                          href="https://www.coursera.org/account/accomplishments/specialization/certificate/ZUNADAFYKHPH">[Credential]</a>-->
<!--                      </li>-->
<!--                      <li>-->
<!--                        <strong>Self-Driving Cars</strong> a 4-course specialization by University of Toronto on-->
<!--                        Coursera. Specialization Certificate earned on June 5, 2019. <a-->
<!--                          href="https://www.coursera.org/account/accomplishments/specialization/certificate/78KLAA9EGVRP">[Credential]</a>-->
<!--                      </li>-->
<!--                    </ul>-->
<!--                  </td>-->
<!--                </tr>-->
<!--              </tbody>-->
<!--            </table>-->
<!--          </div>-->

          <!-- Acknowledgements -->
          <table
            style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding-top:1%;padding-bottom:0%;padding-left:0%;padding-right:0%">
                  <p style="color:#333">
                    <strong>Updated</strong> November 22 2023
                  </p>
                </td>
                <td style="padding-top:1%;padding-bottom:0%;padding-left:0%;padding-right:0%;text-align:right">
                  <p style="color:#333">
                    Template from <a href="https://mikes96.github.io/">here</a>, <a href="https://jonbarron.info/">here</a> and <a
                      href="https://gkioxari.github.io/">here</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
  <script xml:space="preserve" language="JavaScript">
    hideallbibs();
  </script>
</body>

</html>
