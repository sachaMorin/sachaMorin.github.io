<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-175789805-1"></script>
  <script type="text/javascript" src="hidebib.js"></script>
  <!-- Academicons -->
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css" />
  <!-- TO HAVE ICONS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- Style -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href="style.css" rel="stylesheet" type="text/css" />
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-175789805-1');
  </script>

  <title>Miguel Saavedra-Ruiz</title>

  <meta name="author" content="Miguel Saavedra-Ruiz">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords"
    content="Miguel, Saavedra, Saavedra-Ruiz, Robotics, Robot, Deep, Learning, Reinforcement, Computer, Vision, Self-Driving, Vehicles, Mobile, Sensor, Fusion">
</head>

<body>

  <!-- Banner -->
  <table
    style="width:100%;max-width:900px;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding: 0;">
        <td style="padding:0">

          <div class="containersmall">
            <table
              style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tr style="padding:0">
                <!--<td style="padding:2.5%;width:20%;vertical-align:middle;min-width:100px">-->
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="index.html" target="_self" style="font-size:22px">About me</a>
                </td>
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="#news" target="_self" style="font-size:22px">News</a>
                </td>
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="#pub_sec" target="_self" style="font-size:22px">Publications</a>
                </td>
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="#proj_sec" target="_self" style="font-size:22px">Projects</a>
                </td>
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="#others" target="_self" style="font-size:22px">Certificates</a>
                </td>
              </tr>
            </table>
          </div>
          <br>

          <!-- Intro -->
          <div class="containersmall">
            <table
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0">
                  <td
                    style="padding-top:1%;padding-bottom:1%;padding-left:4%;padding-right:4%;width:67%;vertical-align:middle">
                    <p style="text-align:center">
                      <name>Miguel Saavedra-Ruiz</name>
                    </p>
                    <p>
                      I am a Research Master's student with emphasis in Artificial Intelligence and Robotics advised by
                      <a href="https://liampaull.ca/">Liam Paull</a> in
                      <a href="https://montrealrobotics.ca/">Robotics and Embodied AI Lab (REAL)</a> at <a
                        href="https://www.umontreal.ca/">Université de Montréal</a> and <a
                        href="https://mila.quebec/en/">MILA</a>.
                      Previously, I obtained a Postgraduate Diploma in Artificial Intelligence and a BEng degree as a
                      Mechatronics Engineer from <a href="https://www.uao.edu.co/">Universidad Autonoma de Occidente
                        (UAO)</a>
                      in Cali, Colombia.
                    </p>
                    <p>
                      During the BEng degree, I worked under the supervision of <a
                        href="https://scholar.google.com.au/citations?user=x3M1JlAAAAAJ&hl=en"> Victor Romero-Cano</a>
                      on different areas such as object detection and tracking, mobile robotics, sensor fusion and deep
                      neural networks.
                      My undergraduate <a href="http://red.uao.edu.co/bitstream/10614/10754/5/T08388.pdf">degree
                        project</a> was entitled
                      "Autonomous landing system for an unmanned aerial vehicle on a terrestrial vehicle" and consisted
                      in the development
                      of the vision and control pipelines to autonomously land a UAV on a ground vehicle.
                    </p>
                    <p>
                      Over the past two years, my research has been focused on areas such as Reinforcement Learning,
                      Bayesian Inference,
                      Object Detection, and the application of AI in robotics-vision applications. Formerly I worked as
                      a Machine Learning
                      Engineer at <a href="https://whaleandjaguar.co/">Whale & Jaguar</a> (NLP) and as an R&D
                      Robotics Software Engineer at <a href="http://romerocanoingenieria.com/"><strong>R</strong>omero
                        <strong>C</strong>ano <strong>I</strong>ngenieria</a>
                      (Robotics vision, Multi-modal sensor fusion, AI, and UAVs.)
                    </p>
                    <!-- MSc opportunity!!!
                    <p>
                      <span class="highlight"><strong>I am looking for an exciting MSc or MASc opportunity in Computer vision, Robotics, AI or RL where I can contribute into these fascinating fields.</strong></span>
                    </p>
                    -->
                    <p style="text-align:center">
                      <a href="mailto:mguel.angel.saavedra.ruiz@umontreal.ca"><i style="font-size:16px"
                          class="fa fa-envelope"></i> &nbsp miguel [dot] angel [dot] saavedra [dot] ruiz [at] umontreal
                        [dot] ca &nbsp <i style="font-size:16px" class="fa fa-envelope"></i></a>
                    </p>
                    <hr>
                    <!-- Icons -->
                    <p style="text-align:center">
                      <a href="https://github.com/MikeS96"><span style="font-size:24px"
                          class="social-icon fa fa-github"></span></a> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp
                      <!-- <a href=""><span  class="social-icon fa fa-skype"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  -->
                      <a href="data/MiguelSaavedra_CV.pdf" style="font-size:24px" class="links"> CV </a> &nbsp &nbsp
                      &nbsp &nbsp &nbsp &nbsp
                      <a href="https://scholar.google.com/citations?user=tK9Tln0AAAAJ&hl=en"><span
                          style="font-size:24px" class="ai ai-google-scholar ai"></span></a> &nbsp &nbsp &nbsp &nbsp
                      &nbsp &nbsp
                      <!-- <a href=""><span  class="social-icon fa fa-instagram"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  -->
                      <a href="https://www.linkedin.com/in/miguel-a-saavedra-ruiz"><span style="font-size:24px"
                          class="social-icon fa fa-linkedin"></span></a>
                    </p>
                    <hr>
                  </td>
                  <td style="padding:2.5%;width:33%;max-width:33%">
                    <a href="https://www.linkedin.com/in/miguel-a-saavedra-ruiz"><img style="width:100%;max-width:100%"
                        alt="profile photo" src="images/miguel_circle.png" class="hoverZoomLink"></a>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <br>

          <!-- Research -->
          <div class="containersmall">
            <table id="research_sec"
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td
                    style="padding-top:1%;padding-bottom:1%;padding-left:4%;padding-right:4%;width:100%;vertical-align:middle;">
                    <p style="text-align:center">
                      <heading>Research</heading>
                    </p>
                    <p>
                      I am broadly interested in the areas of <strong>Robotics vision</strong>, <strong>Self-Supervised
                      Representation Learning</strong>, <strong>Computer vision</strong>, <strong>SLAM</strong>,
                      <strong>Graphical Models</strong>, <strong>Uncertainty Estimation</strong>, <strong>Reinforcement
                      Learning</strong>, <strong>Graphical models</strong>. Most of my work
                      is motivated by the following question: "How can robotic agents be endowed with environmental
                      awareness to start perceiving their surroundings accurately and reliably and thus, enhance their
                      decision-making capabilities?"
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <br>

          <!-- News -->
          <div class="containersmall" id="news">
            <table
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td
                    style="padding-top:1%;padding-bottom:1%;padding-left:4%;padding-right:4%;width:100%;vertical-align:middle">
                    <p style="text-align:center">
                      <heading>News</heading>
                    </p>
                    <ul>
                      <li>
                        <span style="color:#ff0000"><strong>[September 2021]</strong></span> I Started a research Master
                        in Computer Science at <a href="https://mila.quebec/en/">MILA</a> and <a
                          href="https://www.umontreal.ca/">Université de Montréal</a> under the supervision of <a
                          href="https://liampaull.ca/">Liam Paull</a>. The focus will be in Artificial Intelligence and
                        Robotics.
                      </li>
                      <li>
                        <span style="color:#ff0000"><strong>[July 2021]</strong></span> The paper <a
                          href="https://arxiv.org/abs/2108.06616">"Monocular visual autonomous landing system for
                          quadcopter drones using software in the loop"</a> was accepted in the journal <a
                          href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=62">Aerospace & Electronics
                          Systems Magazine</a> IEEE.
                      </li>
                      <li>
                        <strong>[July 2021]</strong> Successfully completed a Postgraduate Diploma in Artificial
                        Intelligence at <a href="https://www.uao.edu.co/">Universidad Autonoma de Occidente</a>.
                      </li>
                      <li>
                        <strong>[June 2021]</strong> I will join <a href="https://mila.quebec/en/">MILA</a> and <a
                          href="https://www.umontreal.ca/">Université de Montréal</a>
                        as a MSc student under the supervision of <a href="https://liampaull.ca/">Liam Paull</a>
                        starting in Fall 2021.
                      </li>
                      <li>
                        <strong>[April 2021]</strong> The extended abstract <a
                          href="https://arxiv.org/abs/2105.11060">High-level camera-LiDAR fusion for 3D object detection
                          with
                          machine learning</a> was accepted to the <a
                          href="https://www.latinxinai.org/cvpr-2021-about">LatinX in AI Research Workshop</a> at <a
                          href="http://cvpr2021.thecvf.com/">CVPR</a>, 2021 as a poster presentation.
                      </li>
                      <li>
                        <strong>[December 2020]</strong> I am joining the <a href="https://whaleandjaguar.co/">Whale &
                          Jaguar</a>'s team as a Machine Learning Engineer.
                      </li>
                      <li>
                        <strong>[August 2020]</strong> I have been accepted into the <a
                          href="https://riiaa.org/en/home/">RIIAA</a> online summer school in RL and Bayesian inference
                        as an active track.
                      </li>
                      <li>
                        <strong>[August 2020]</strong> I Started a Postgraduate Diploma in Artificial Intelligence at <a
                          href="https://www.uao.edu.co/">Universidad Autonoma de Occidente</a>, the degree will end in
                        June 2021.
                      </li>
                      <li>
                        <strong>[Jul 2020]</strong> The paper "Monocular visual autonomous landing system for quadcopter
                        drones using software in the loop" has been sent to the journal <a
                          href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=62">Aerospace & Electronics
                          Systems Magazine</a> IEEE (Under review).
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <br>

          <!-- Publications -->
          <div class="containersmall" id="pub_sec">
            <table
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td colspan="2" style="padding:1%;width:100%">
                    <p style="text-align:center">
                      <heading>Publications</heading>
                    </p>
                  </td>
                  <td></td>
                </tr>

                <tr onmouseout="dals_stop()" onmouseover="dals_start()">
                  <td
                    style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='dals_image'>
                        <img src='images/gazebo_dron_after.gif' width="160">
                      </div>
                      <img src='images/gazebo_dron_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function dals_start() {
                        document.getElementById('dals_image').style.opacity = "1";
                      }

                      function dals_stop() {
                        document.getElementById('dals_image').style.opacity = "0";
                      }
                      dals_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/autonomous_landing_uav">
                      <papertitle>
                        Monocular visual autonomous landing system for quadcopter drones using software in the loop
                      </papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>,
                    <a href="https://www.linkedin.com/in/ana-maria-pinto-b380b017a/">Ana Pinto</a>,
                    <a href="https://scholar.google.com.au/citations?user=x3M1JlAAAAAJ&hl=en">Victor Romero-Cano</a>
                    <br>
                    <em>IEEE Aerospace And Electronic Systems </em>, 2021 &nbsp <span
                      style="color:#ff0000"><strong>(Journal publication)</strong></span>
                    <br>

                    <div class="paper" id="dals">
                      <a href="https://github.com/MikeS96/autonomous_landing_uav">code</a> /
                      <a href="data/RedColsiPasto.pdf">poster</a> /
                      <a href="https://red.uao.edu.co/bitstream/10614/10754/5/T08388.pdf">thesis</a> /
                      <a href="https://arxiv.org/abs/2108.06616">arXiv</a> /
                      <a href="https://www.youtube.com/watch?v=2NWR9Otdz-s&feature=youtu.be">video</a> /
                      <a href="javascript:togglebib('dals')" class="togglebib">bibtex</a>

                      <pre xml:space="preserve">
@ARTICLE{9656574,
  author={Saavedra-Ruiz, Miguel and Pinto-Vargas, Ana Maria and Romero-Cano, Victor},
  journal={IEEE Aerospace and Electronic Systems Magazine},
  title={Monocular Visual Autonomous Landing System for Quadcopter Drones using Software in the Loop},
  year={2021},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/MAES.2021.3115208}}
                </pre>
                    </div>
                    <p></p>
                    <div class="paper" id="dals_description">
                      <h6><a href="javascript:togglebib('dals_description')" class="togglebib">Autonomous landing system
                          for a UAV on a terrestrial vehicle using robotics vision and control.</a></h6>
                      <pre xml:space="preserve">
My BEng degree project addressed the problem of the autonomous landing of a UAV
with a landing platform located on the top of a ground vehicle. The project
utilized vision-based techniques to detect the landing platform, a Kalman
filter was tailored for the tracking phase and finally, a PID controller sent
control commands to the flight controller of the UAV to land properly on the
platform. Rigorous assessments were conducted through the simulation of the
whole robotic stack with ROS and gazebo in the software in the loop provided by
PX4. Ultimately, the system was tested in a custom DJI F-450 and embedded in a
Odroid XU4. The system demonstrates a satisfactory performance and was able to
land with a mean error of ten centimeters from the center of the landing
platform (Implemented in Python, C++/Linux).

Autonomous landing is a capability that is essential to achieve the full potential
of multi-rotor drones in many social and industrial applications. The implementation
and testing of this capability on physical platforms is risky and resource-intensive;
hence, in order to ensure both a sound design process and a safe deployment,
simulations are required before implementing a physical prototype. This paper presents
the development of a monocular visual system, using a software-in-the-loop methodology,
that autonomously and efficiently lands a quadcopter drone on a predefined landing pad,
thus reducing the risks of the physical testing stage. In addition to ensuring that
the autonomous landing system as a whole fulfils the design requirements using a
Gazebo-based simulation, our approach provides a tool for safe parameter tuning and
design testing prior to physical implementation. Finally, the proposed monocular
vision-only approach to landing pad tracking made it possible to effectively implement
the system in an F450 quadcopter drone with the standard computational capabilities of
an Odroid XU4 embedded processor.
                </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>


                <tr onmouseout="ml3d_stop()" onmouseover="ml3d_start()" style="border-bottom: 5px solid #000;">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='ml3d_image'>
                        <img src='images/3d_after.gif' width="160">
                      </div>
                      <img src='images/3d_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function ml3d_start() {
                        document.getElementById('ml3d_image').style.opacity = "1";
                      }

                      function ml3d_stop() {
                        document.getElementById('ml3d_image').style.opacity = "0";
                      }
                      ml3d_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/3d_obj_detection">
                      <papertitle>3D object detector for vehicles using classic Machine Learning</papertitle>
                    </a>
                    <br>
                    <a href="https://www.linkedin.com/in/gustavo-andres-salazar-gomez/">Gustavo Salazar</a>,
                    <strong>Miguel Saavedra-Ruiz</strong>,
                    <a href="https://scholar.google.com.au/citations?user=x3M1JlAAAAAJ&hl=en">Victor Romero-Cano</a>
                    <br>
                    <em>LatinX Workshop at CVPR</em>, 2021 &nbsp <span style="color:#ff0000"><strong>(Poster
                        Presentation)</strong></span>
                    <br>
                    <div class="paper" id="ml3d">
                      <a href="https://github.com/MikeS96/3d_obj_detection">code</a> /
                      <a href="https://arxiv.org/abs/2105.11060">arXiv</a> /
                      <a href="https://research.latinxinai.org/papers/cvpr/2021/png/21_poster_21.png">poster</a>
                      <p></p>

                      <div class="paper" id="ml3d_description">
                        <h6><a href="javascript:togglebib('ml3d_description')" class="togglebib">3D object detection of
                            vehicles in the NuScenes dataset using classic Machine learning such as DBSCAN and SVMs.</a>
                        </h6>
                        <pre xml:space="preserve">
3D object detection is a problem that has gained popularity among the research community due to
its extensiveset of application on autonomous navigation, surveillance and pick-and-place. Most
of the solutions proposed in the state-of-the-art are based on deep learning techniques and
present astonishing results in terms of accuracy. Nevertheless, a set of problems inherits from
this sort of solutions such as the need of enormous tagged datasets, extensive computational
resources due to the complexity of the model and most of the time, no real-time inference.
This work proposes an end-to-end classic Machine Learning (ML) pipeline to solve the
3D object detection problem for cars. The proposed method is leveraged on the use of frustum
region proposals to segment and estimate the parameters of the amodal 3D bouning box. Here we
do not deal with the problem of 2D object detection as for most of the research community this
is considered solved with ConvolutionalNeural Networks (CNN).

This task is addressed employing different ML techniques such as RANSAC for road segmentation and
DBSCAN for clustering. Global features are extracted out of the segmented point cloud using
The Ensemble of Shape Functions (ESF). Some feature are engineered through PCA and statistics.
Finally, the amodal 3D bounding box parameters are estimated through a SVR regressor.
                </pre>
                      </div>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <tr onmouseout="dtl_stop()" onmouseover="dtl_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='dtl_image'>
                        <img src='images/detection_ccra_after.png' width="160">
                      </div>
                      <img src='images/detection_ccra_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function dtl_start() {
                        document.getElementById('dtl_image').style.opacity = "1";
                      }

                      function dtl_stop() {
                        document.getElementById('dtl_image').style.opacity = "0";
                      }
                      dtl_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/autonomous_landing_uav/tree/master/object_detector">
                      <papertitle>Detection and tracking of a landing platform for aerial robotics applications
                      </papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>,
                    <a href="https://www.linkedin.com/in/ana-maria-pinto-b380b017a/">Ana Pinto</a>,
                    <a href="https://scholar.google.com.au/citations?user=x3M1JlAAAAAJ&hl=en">Victor Romero-Cano</a>
                    <br>
                    <em>CCRA</em>, 2018 &nbsp <span style="color:#ff0000"><strong>(Oral Presentation)</strong></span>
                    <br>
                    <div class="paper" id="dtl">
                      <a href="https://github.com/MikeS96/autonomous_landing_uav/tree/master/object_detector">code</a> /
                      <a href="https://www.youtube.com/watch?v=knH2Hju--3I&feature=emb_title">video</a> /
                      <a href="javascript:togglebib('dtl')" class="togglebib">bibtex</a>
                      <pre xml:space="preserve">
@INPROCEEDINGS{8588112,
  author={M. S. {Ruiz} and A. M. P. {Vargas} and V. R. {Cano}},
  booktitle={2018 IEEE 2nd Colombian Conference on Robotics and Automation (CCRA)},
  title={Detection and tracking of a landing platform for aerial robotics applications},
  year={2018},
  volume={},
  number={},
  pages={1-6},
}
              </pre>
                      <p></p>
                      <div class="paper" id="dtl_description">
                        <h6><a href="javascript:togglebib('dtl_description')" class="togglebib">Object Detection and
                            tracking pipelines to detect a landing pad on the ground from a UAV.</a></h6>
                        <pre xml:space="preserve">
Aerial robotic applications need to be endowed with systems capable to accurately locate objects of
interest to perform specific tasks at hand. I Developed an embedded vision-based landing platform
detection and tracking system with ROS and OpenCV. The system extended the capabilities of a
SURF-based feature detector-descriptor that makes detections of a landing pad alongside a Kalman
filter-based estimation module. The system demonstrated a considerable improvement over
only-detector methods, diminishing the detection error and providing accurate estimations of the
landing pad position (Implemented in C++/Linux).
              </pre>
                      </div>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <tr onmouseout="ldp_stop()" onmouseover="ldp_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='ldp_image'>
                        <img src='images/localization_after.gif' width="160">
                      </div>
                      <img src='images/localization_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function ldp_start() {
                        document.getElementById('ldp_image').style.opacity = "1";
                      }

                      function ldp_stop() {
                        document.getElementById('ldp_image').style.opacity = "0";
                      }
                      ldp_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/autonomous_landing_uav/tree/master/object_detector">
                      <papertitle>Localization of a landing platform for a UAV</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>,
                    <a href="http://www.seanfanello.it/">Ana Pinto</a>,
                    <a href="https://research.google/people/105312/">Victor Romero-Cano</a>
                    <br>
                    <span style="color:#ff0000"><strong>(Poster Presentation)</strong></span>
                    <br>
                    <a href="https://github.com/MikeS96/autonomous_landing_uav/tree/master/object_detector">code</a> /
                    <a href="data/pendonUAO2018.pdf">poster</a>
                    <p></p>
                    <p>Localization of a landing pad located at the top of a ground vehicle with a UAV.</p>
                  </td>
                </tr>

              </tbody>
            </table>
          </div>
          <br>

          <!-- Projects -->
          <div class="containersmall" id="proj_sec">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <tr>
                  <td colspan="2" style="padding:2%;width:100%">
                    <p style="text-align:center">
                      <heading>Projects</heading>
                    </p>
                  </td>
                  <td></td>
                </tr>

                <!-- DINO Real Project -->
                <tr onmouseout="dino_real_stop()" onmouseover="dino_real_start()">
                  <td
                    style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='dino_real_seg'>
                        <img src='images/dino_real_after.gif' width="160">
                      </div>
                      <img src='images/dino_real_before.jpg' width="160">
                    </div>
                    <script type="text/javascript">
                      function dino_real_start() {
                        document.getElementById('dino_real_seg').style.opacity = "1";
                      }

                      function dino_real_stop() {
                        document.getElementById('dino_real_seg').style.opacity = "0";
                      }
                      dino_real_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://sachamorin.github.io/dino/">
                      <papertitle>Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers
                      </papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <a href="https://sachamorin.github.io/dino/">Sacha Morin</a>,
                    <a href="https://liampaull.ca/">Liam Paull</a>
                    <br>
                    <a href="https://github.com/sachaMorin/dino">code (model)</a> /
                    <a href="https://github.com/MikeS96/object-detection/tree/daffy">code (servoing)</a> /
                    <a href="https://arxiv.org/abs/2203.03682">arXiv</a> /
                    <a href="https://sachamorin.github.io/dino/">webpage</a>
                    <p></p>
                    <p>Visual Servoing navigation using pre-trained Self-Supervised Vision Transformers.</p>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- DINO Simulated Project -->
                <tr onmouseout="dino_stop()" onmouseover="dino_start()">
                  <td
                    style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='dino_seg'>
                        <img src='images/dino_after.gif' width="160">
                      </div>
                      <img src='images/dino_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function dino_start() {
                        document.getElementById('dino_seg').style.opacity = "1";
                      }

                      function dino_stop() {
                        document.getElementById('dino_seg').style.opacity = "0";
                      }
                      dino_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/sachaMorin/dino">
                      <papertitle>Self-Supervised Learning : Applications to object detection and image segmentation
                      </papertitle>
                    </a>
                    <br>
                    <a href="https://github.com/sachaMorin">Sacha Morin</a>,
                    <strong>Miguel Saavedra-Ruiz</strong>,
                    Wei Fen
                    <br>
                    <a href="https://github.com/sachaMorin/dino">code</a>
                    <p></p>
                    <p>DINO features for object detection and sparse instance segmentation in duckie-town dataset.</p>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Transfer Learning -->
                <tr onmouseout="style_stop()" onmouseover="style_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='style_image'>
                        <img src='images/style_after.gif' width="160">
                      </div>
                      <img src='images/style_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function style_start() {
                        document.getElementById('style_image').style.opacity = "1";
                      }

                      function style_stop() {
                        document.getElementById('style_image').style.opacity = "0";
                      }
                      style_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/zestian56/style-transfer">
                      <papertitle>Style-transfer for the creation of aesthetic images</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>,
                    <a href="https://github.com/gsg213">Gustavo Salazar</a>,
                    <a href="https://github.com/zestian56">Sebastian Botero</a>
                    <br>
                    <a href="https://github.com/zestian56/style-transfer">code</a> /
                    <a href="data/styleTransfer.pdf">report (spanish)</a>
                    <p></p>
                    <div class="paper" id="style_description">
                      <h6><a href="javascript:togglebib('style_description')" class="togglebib">Style-transfer
                          implementarion based on the paper A neural algorithm of artistic style using VGG-19.</a></h6>
                      <pre xml:space="preserve">
Implementation of Style-Transfer based on the original paper 'A neural algorithm of artistic style'.
As stated by the authors, style-transfer is 'an artificial system based on a Deep Neural Network
that creates artistic images of high perceptual quality'. In this work we aimed to replicate the
original paper in a Pytorch-based implementation using as backbone a VGG-19 model. The results
present how the implementation is capable to transfer the desired content and style to a different
image and even create an aesthetic result from scratch.

This work was locally deployed using docker-compose and web sockets in order the create a
seamlessly implementation of style-transfer with an intuitive GUI.
                </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Robotics Software -->
                <tr onmouseout="rsnd_stop()" onmouseover="rsnd_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='rsnd_image'>
                        <img src='images/RTAB_after.gif' width="160">
                      </div>
                      <img src='images/RTAB_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function rsnd_start() {
                        document.getElementById('rsnd_image').style.opacity = "1";
                      }

                      function rsnd_stop() {
                        document.getElementById('rsnd_image').style.opacity = "0";
                      }
                      rsnd_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/robotics_software_nd">
                      <papertitle>Robotics Software Engineer projects</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="https://github.com/MikeS96/robotics_software_nd">code</a>
                    <p></p>
                    <div class="paper" id="rsnd_description">
                      <h6><a href="javascript:togglebib('rsnd_description')" class="togglebib">Robot localization,
                          Mapping, SLAM, path planning and navigation.</a></h6>
                      <pre xml:space="preserve">
Implementation of different robotics vision projects. These are based on the robotics
software engineer nanodegree provided by Udacity. All the projects were developed with ROS
and tested in a gazebo-based simulated environment. Different topics were addressed
such as Gazebo basic, ROS, robot localization, Mapping, SLAM, navigation and
path planning. The projects are listed below

  * How to use the model and building editor, plugins in Gazebo and more.
  * Creating a ball chaser robot in ROS and Gazebo.
  * Localization through Kalman Filter, Monte Carlo methods and ACML.
  * Occupancy grid map generation, GridBased Fast Slam ROS package and RTAB-Map SLAM.
  * The A* algorithm and robot navigation.
                </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- VO and VIO -->
                <tr onmouseout="vio_stop()" onmouseover="vio_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='vio_image'>
                        <img src='images/vio_after.gif' width="160">
                      </div>
                      <img src='images/vio_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function vio_start() {
                        document.getElementById('vio_image').style.opacity = "1";
                      }

                      function vio_stop() {
                        document.getElementById('vio_image').style.opacity = "0";
                      }
                      vio_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/vio_quadrotor">
                      <papertitle>Stereo Visual Odometry (VO) and Visual Inertial Odometry (VIO) with EFK in a
                        quad-rotor</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="https://github.com/MikeS96/vio_quadrotor">code</a>
                    <p></p>

                    <div class="paper" id="vio_description">
                      <h6><a href="javascript:togglebib('vio_description')" class="togglebib">Stereo Visual Odometry and
                          Visual Inertial Odometry pipelines to estimate the pose of a quad-rotor.</a></h6>
                      <pre xml:space="preserve">
Implemented Stereo Visual Odometry and Visual Inertial Odometry pipelines to estimate the pose of a quad-rotor.
The system works by taking subsequent image pairs and matching features throughout the test. Once those features
are obtained, 3d-points coordinates were retrieved with the depth map of the images and the extrinsic camera
calibration  matrix. Finally, the trajectory is estimated using 3D-2D Perspective-n-Point (PNP). As an additional
step, the VO trajectory was used with the IMU data in an Error-State Extended Kalman Filter to estimate the pose
even when most of the VO observations were dropped. Both VO and VIO showed good results estimating the trajectory
of the UAV (Implemented in Python/Linux).
                </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- SITL -->
                <tr onmouseout="sitl_stop()" onmouseover="sitl_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='sitl_image'>
                        <img src='images/simu_landing_after.gif' width="160">
                      </div>
                      <img src='images/simu_landing_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function sitl_start() {
                        document.getElementById('sitl_image').style.opacity = "1";
                      }

                      function sitl_stop() {
                        document.getElementById('sitl_image').style.opacity = "0";
                      }
                      sitl_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/autonomous_landing_uav">
                      <papertitle>Simulation of a landing system for a UAV in Gazebo</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="https://github.com/MikeS96/autonomous_landing_uav">code</a> /
                    <a href="https://www.youtube.com/watch?v=41xJj8g8Ce8&feature=youtu.be">video</a>
                    <p></p>
                    <div class="paper" id="sitl_description">
                      <h6><a href="javascript:togglebib('sitl_description')" class="togglebib">Simulation of an
                          autonomous landing system for a UAV with Gazebo, ROS and the Software in the loop provided by
                          PX4.</a></h6>
                      <pre xml:space="preserve">
Simulated an autonomous landing system for a UAV with Gazebo, ROS and the Software in the loop
provided by PX4. The project consisted in the development of different packages in C++ and Python
which allowed the assessment of an autonomous landing system. This robotics simulation allowed the
thoroughly development and evaluation of a landing pipeline for a UAV (Implemented in Python, C++/
Linux).
                </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- RL Projects -->
                <tr onmouseout="rlSpe_stop()" onmouseover="rlSpe_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='rlSpe_image'>
                        <img src='images/lunar_lander_after.gif' width="160">
                      </div>
                      <img src='images/lunar_lander_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function rlSpe_start() {
                        document.getElementById('rlSpe_image').style.opacity = "1";
                      }

                      function rlSpe_stop() {
                        document.getElementById('rlSpe_image').style.opacity = "0";
                      }
                      rlSpe_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/rl_openai">
                      <papertitle>​Reinforcement Learning Specialization Projects</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="https://github.com/MikeS96/rl_openai">code</a> /
                    <a href="https://www.youtube.com/watch?v=rJE_hfkvKvk&feature=youtu.be">video</a>
                    <p></p>
                    <div class="paper" id="rlSpe_description">
                      <h6><a href="javascript:togglebib('rlSpe_description')" class="togglebib">Lunar Lander, Mountain
                          Car and Pendulum classical control tasks solved using RL.</a></h6>
                      <pre xml:space="preserve">
Trained a lunar lander in a simulated environment with Reinforcement Learning.  The agent was
implemented with the Expected-Sarsa algorithm and used a Neural networkfor for action-values
approximation. The algorithm was capable to do planning steps with experience replay and learn a
policy for the landing of the agent. Thorughout the specialization I implemented different
projects, some of those are listed below.

  * Solved a Gridworld city with Dynamic programming to find an optimal policie.
  * Implemented a Dyna-Q and Dyna-Q+ algorithms in a changing maze environment to assess the
  performance of planning methods in RL.
  * Implemented an Average Reward Softmax Actor-Critic algorithm using Tile-coding to solve the
  Pendulum Swing-Up continuous problem.
  * Solved the Mountain car and Lunar Lander problems.
                </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Teleop -->
                <tr onmouseout="teleop_stop()" onmouseover="teleop_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='teleop_image'>
                        <img src='images/teleop_after.gif' width="160">
                      </div>
                      <img src='images/teleop_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function teleop_start() {
                        document.getElementById('teleop_image').style.opacity = "1";
                      }

                      function teleop_stop() {
                        document.getElementById('teleop_image').style.opacity = "0";
                      }
                      teleop_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/autonomous_landing_uav/tree/master/key_teleop">
                      <papertitle>Teleoperation system for a car-like robot</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="https://github.com/MikeS96/autonomous_landing_uav/tree/master/key_teleop">code</a> /
                    <a href="https://www.youtube.com/watch?v=54QGT7Ud98s&feature=youtu.be">video</a>
                    <p></p>

                    <div class="paper" id="teleop_description">
                      <h6><a href="javascript:togglebib('teleop_description')" class="togglebib">Teleoperation system
                          for a car-like robot through mathematical modelling.</a></h6>
                      <pre xml:space="preserve">
Mathematical modelling of unmanned groundvehicles (UGV) is a well studied problem in robotics and
essential for the control of a robot. Developed a teleoperation system for a car-like robot. The
system received velocities in the local coordinate frame of the robot and through the inverse
kinematics model of the vehicle these velocity commands were transformed to wheels’ speed and sent
to the vehicle’s motors. This project was embedded in a Raspberry Pi3 to allow the remote control
of the vehicle with a host computer through WIFI (Implemented in C/Linux).
                </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Turtlebot -->
                <tr onmouseout="mturtle_stop()" onmouseover="mturtle_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='mturtle_image'>
                        <img src='images/map_turtle_after.png' width="160">
                      </div>
                      <img src='images/map_turtle_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function mturtle_start() {
                        document.getElementById('mturtle_image').style.opacity = "1";
                      }

                      function mturtle_stop() {
                        document.getElementById('mturtle_image').style.opacity = "0";
                      }
                      mturtle_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="data/slidesturtle.pdf">
                      <papertitle>Mapping and localization in indoors with Turtlebot 2</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="data/slidesturtle.pdf">slides</a> /
                    <a href="data/finalreport.pdf">report</a>
                    <p></p>
                    <div class="paper" id="mturtle_description">
                      <h6><a href="javascript:togglebib('mturtle_description')" class="togglebib">Simulation of a
                          localization and mapping system (laser-based SLAM) for a turtlebot2 in indoors.</a></h6>
                      <pre xml:space="preserve">
Equipping robotic systems with novel localization and navigation stacks is crucial for autonomous
navigation. A localization and mapping system (laser-based SLAM) for a turtlebot2 in indoors was
simulated. The system was capable to accurately localize the robot in a previously mapped
environment and subsequently navigate to a specific position in an occupancy grid map (Implemented
in C++/Linux).
                </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Self-Driving Cars Specialization -->
                <tr onmouseout="carsSpe_stop()" onmouseover="carsSpe_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='carsSpe_image'>
                        <img src='images/cars_after.gif' width="160">
                      </div>
                      <img src='images/cars_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function carsSpe_start() {
                        document.getElementById('carsSpe_image').style.opacity = "1";
                      }

                      function carsSpe_stop() {
                        document.getElementById('carsSpe_image').style.opacity = "0";
                      }
                      carsSpe_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://www.youtube.com/watch?v=f-k9wR0Ty8o">
                      <papertitle>Self-Driving Cars Specialization Projects</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="https://www.youtube.com/watch?v=f-k9wR0Ty8o">video1</a> /
                    <a href="https://www.youtube.com/watch?v=1I6UJQyJovs">video2</a>
                    <p></p>

                    <div class="paper" id="carsSpe_description">
                      <h6><a href="javascript:togglebib('carsSpe_description')" class="togglebib">Wheeled-robot
                          mathematical modelling through dynamical modelling (tire model), lateral and longitudinal
                          control, state estimation with Kalman filters, visual perception and motion planning for
                          self-driving vehicles.</a></h6>
                      <pre xml:space="preserve">
This work involved four capstone projects in the area of self-driving cars. Topics such as
wheeled-robot mathematical modelling through dynamical modelling (tire model), lateral and
longitudinal control, state estimation with Kalman filters, visual perception and motion planning
were addressed. Most of the projects were tested in the Carla Simulator to assess performance. A
description of the projects developed are presented below.

* Control of a car-like robot through a longitudinal and lateral controller. The longitudinal
controller was implemented with a PID and the lateral controller was a cross-track error
controller.
* Implementation of an error state extended Kalman Filter for the estimation of the trajectory
of a vehicle. The filter fused information from a GNSS and IMU alongside the dynamic model of
the vehicle to produce an accurate estimation of its trajectory on the space.
* Robotics perception stack which detected the drivable space of the vehicle through image
segmentation. Canny edge detector was used to detect the lines of the road and a depth
representation of the scene was employed to estimate the distance-to-objects in the road and
avoid collision using only image-based methods.
* Implemented a navigation stack in the Carla simulator with the use of grid world
representations  and state machines for a simple navigation strategy.
                </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- License Plate Project -->
                <tr onmouseout="lplate_stop()" onmouseover="lplate_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='lplate_image'>
                        <img src='images/license_plate_after.png' width="160">
                      </div>
                      <img src='images/license_plate_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function lplate_start() {
                        document.getElementById('lplate_image').style.opacity = "1";
                      }

                      function lplate_stop() {
                        document.getElementById('lplate_image').style.opacity = "0";
                      }
                      lplate_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="data/licensePlate.txt">
                      <papertitle>​Low-cost license plate recognition system based on CNN</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <p></p>

                    <div class="paper" id="lplate_description">
                      <h6><a href="javascript:togglebib('lplate_description')" class="togglebib">Implemented a low-cost
                          license plate recognition systems using deep learning techniques.</a></h6>
                      <pre xml:space="preserve">
Automatic license plate recognition (LPR) is indispensable for the admission and flow control of
vehicles into parking lots orcondominiums.  Generally, these systems are based on classic computer
vision techniques owing to their processing speed, however, these approaches can lead to inaccurate
detections and vague performance on non-ideal environmental conditions. My work tried to surpass
these setbacks with the implementation of an image-based plate recognition system using
convolutional neural networks (CNN) to enhance the current methods. The system was optimized and
embedded in a Nvidia Jetson Nano to run in a low-cost computer at a recognition rate of 100ms per
plate making it ideal to operate in the places mentioned before (Implemented in Python, C++,
TensorRT/Linux).
                </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Flow-Control with heatmaps -->
                <tr onmouseout="flowc_stop()" onmouseover="flowc_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='flowc_image'>
                        <img src='images/flow_control_after.png' width="160">
                      </div>
                      <img src='images/flow_control_before.jpg' width="160">
                    </div>
                    <script type="text/javascript">
                      function flowc_start() {
                        document.getElementById('flowc_image').style.opacity = "1";
                      }

                      function flowc_stop() {
                        document.getElementById('flowc_image').style.opacity = "0";
                      }
                      flowc_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="data/flowControl.txt">
                      <papertitle>Flow control with heatmaps in indoors</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <p></p>

                    <div class="paper" id="flowc_description">
                      <h6><a href="javascript:togglebib('flowc_description')" class="togglebib">Heat map generator based
                          on computer vision techniques to stochastically estimate the most visited areas in an indoor
                          space with a monocular camera.</a></h6>
                      <pre xml:space="preserve">
Implemented a heat map generator based on computer vision techniques to stochastically estimate the
most visited areas in an indoor space with a monocular camera. A feature tracker method was used to
estimate the average flow of persons and a deep convolutional neural network was employed to obtain
the segmentation of the floor in the scene. This information was merge together to ​gather relevant
information about the people habits in shopping centers or crowded areas (Implemented in Python/
Linux)
                </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Calendula Flower Experiments -->
                <tr onmouseout="calenF_stop()" onmouseover="calenF_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='calenF_image'>
                        <img src='images/calendula_after.jpg' width="160">
                      </div>
                      <img src='images/calendula_after.jpg' width="160">
                    </div>
                    <script type="text/javascript">
                      function calenF_start() {
                        document.getElementById('calenF_image').style.opacity = "1";
                      }

                      function calenF_stop() {
                        document.getElementById('calenF_image').style.opacity = "0";
                      }
                      calenF_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="data/calendulaFlower.txt">
                      <papertitle>​Calendula Flower Classification System</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <p></p>

                    <div class="paper" id="calenF_description">
                      <h6><a href="javascript:togglebib('calenF_description')" class="togglebib">Implementation of a
                          low-cost Calendula flower classification system using CNN.</a></h6>
                      <pre xml:space="preserve">
Implemented a low cost Calendula flower classification system with Tensorflow, Android Studio and
Arduino. The system used a deep convolutional neural network inception v3 trained with different
Calendula flowers. When the system detected a Calendula flower, it sent a signal to an Arduino
board and moved a servomotor to a specific point which allowed the correct classification of the
flowers through a transportation band (Implemented in C, Python/Linux).
                </pre>
                    </div>
                  </td>
                </tr>

              </tbody>
            </table>
          </div>
          <br>

          <!-- Certificates -->
          <div class="containersmall" id="others">
            <table
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td
                    style="padding-top:1%;padding-bottom:1%;padding-left:4%;padding-right:4%;width:100%;vertical-align:middle">
                    <p style="text-align:center" id="cert_sec">
                      <heading>Courses and Certifications</heading>
                    </p>
                    <ul>
                      <li>
                        <strong>Reinforcement Learning</strong> by University of Alberta & Alberta Machine Intelligence
                        Institute on Coursera. Certificate earned at June 21, 2020. <a
                          href="https://www.coursera.org/account/accomplishments/specialization/certificate/ZUNADAFYKHPH">[Credential]</a>
                      </li>
                      <li>
                        <strong>Self-Driving Cars</strong> a 4-course specialization by University of Toronto on
                        Coursera. Specialization Certificate earned on June 5, 2019. <a
                          href="https://www.coursera.org/account/accomplishments/specialization/certificate/78KLAA9EGVRP">[Credential]</a>
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>

          <!-- Acknowledgements -->
          <table
            style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding-top:1%;padding-bottom:0%;padding-left:0%;padding-right:0%">
                  <p style="color:#333">
                    <strong>Updated</strong> March 20th 2022
                  </p>
                </td>
                <td style="padding-top:1%;padding-bottom:0%;padding-left:0%;padding-right:0%;text-align:right">
                  <p style="color:#333">
                    This template was shamelessly stolen from <a href="https://jonbarron.info/">here</a> and <a
                      href="https://gkioxari.github.io/">here</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
  <script xml:space="preserve" language="JavaScript">
    hideallbibs();
  </script>
</body>

</html>
