<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-175789805-1"></script>
  <script type="text/javascript" src="hidebib.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.10/dist/clipboard.min.js"></script>
  <!-- Academicons -->
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css" />
  <!-- TO HAVE ICONS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- Style -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href="style.css" rel="stylesheet" type="text/css" />
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-175789805-1');
  </script>

  <title>Sacha Morin</title>

  <meta name="author" content="Sacha Morin">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords"
    content="Sacha, Morin, Morin, Robotics, Robot, Deep, Learning, Self-Driving, Vehicles, Mobile, Sensor, Fusion, Manifold, Geometric, Self-Supervised Learning, Unsupervised">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü§ñ</text></svg>">
  <meta property="og:type" content="website"/>
</head>

<body>

  <!-- Banner -->
  <table
    style="width:100%;max-width:900px;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding: 0;">
        <td style="padding:0">

          <div class="containersmall">
            <table
              style="width:80%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tr style="padding:0">
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="index.html" target="_self" style="font-size:22px">About me</a>
                </td>
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="#news" target="_self" style="font-size:22px">News</a>
                </td>
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="#pub_sec" target="_self" style="font-size:22px">Publications</a>
                </td>
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="#proj_sec" target="_self" style="font-size:22px">Projects</a>
                </td>
                <td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
                  <a href="#others" target="_self" style="font-size:22px">Certificates</a>
                </td>
              </tr>
            </table>
          </div>
          <br>

          <!-- Intro -->
          <div class="containersmall">
            <table
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0">
                  <td
                    style="padding-top:1%;padding-bottom:1%;padding-left:4%;padding-right:4%;width:67%;vertical-align:middle">
                    <p style="text-align:center">
                      <name>Sacha Morin</name>
                    </p>
                    <p>
                      I am a PhD student in Machine Learning at <a
                        href="https://www.umontreal.ca/">Universit√© de Montr√©al</a> and <a
                        href="https://mila.quebec/en/">MILA</a>. I am advised by
                      <a href="https://www.guywolf.org">Guy Wolf</a> and part of the <a href="https://www.diffusion.space">RAFALES</a> lab.
                    </p>
                    <p>I obtained a BSc in Mathematics and Computer Science from the Universit√© de Montr√©al in 2021 and a Bachelor of Law from
                    the <a href="https://www.usherbrooke.ca/">Universit√© de Sherbrooke</a> in 2017.</p>
                    <p style="text-align:center">
                      <a href="mailto:sacha.morin@mila.quebec"><i style="font-size:16px"
                          class="fa fa-envelope"></i> &nbsp sacha.morin@mila.quebec &nbsp <i style="font-size:16px" class="fa fa-envelope"></i></a>
                    </p>
                    <hr>
                    <!-- Icons -->
                    <p style="text-align:center">
                      <a href="https://github.com/sachaMorin"><span style="font-size:24px"
                          class="social-icon fa fa-github"></span></a> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp
                      <!-- <a href=""><span  class="social-icon fa fa-skype"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  -->
                      <a href="data/sachamorin_cv.pdf" style="font-size:24px" class="links"> CV </a> &nbsp &nbsp
                      &nbsp &nbsp &nbsp &nbsp
                      <a href="https://scholar.google.com/citations?user=mIMinkMAAAAJ&hl=en&oi=ao"><span
                          style="font-size:24px" class="ai ai-google-scholar ai"></span></a> &nbsp &nbsp &nbsp &nbsp
                      &nbsp &nbsp
                      <a href="https://www.linkedin.com/in/sacha-morin-117752124/?locale=en_US"><span style="font-size:24px"
                          class="social-icon fa fa-linkedin"></span></a> &nbsp  &nbsp  &nbsp  &nbsp
                      <a href="https://twitter.com/sachamori"><span style="font-size:24px" class="social-icon fa fa-twitter"></span></a>
                    </p>
                    <hr>
                  </td>
                  <td style="padding:2.5%;width:33%;max-width:33%">
                    <a href="https://www.linkedin.com/in/sacha-morin-117752124/?locale=en_US"><img style="width:100%;max-width:100%"
                        alt="profile photo" src="images/sacha.jpeg" class="hoverZoomLink"></a>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <br>

          <!-- Research -->
          <div class="containersmall">
            <table id="research_sec"
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td
                    style="padding-top:1%;padding-bottom:1%;padding-left:4%;padding-right:4%;width:100%;vertical-align:middle;">
                    <p style="text-align:center">
                      <heading>Research</heading>
                    </p>
                    <p>
                      My research interests include manifold learning, mobile robotics, geometric deep learning and self-supervised learning.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <br>

          <!-- News -->
          <div class="containersmall" id="news">
            <table
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td
                    style="padding-top:1%;padding-bottom:1%;padding-left:4%;padding-right:4%;width:100%;vertical-align:middle">
                    <p style="text-align:center">
                      <heading>News</heading>
                    </p>
                    <ul>
                      <li>
                        <span style="color:#ff0000"><strong>[April 2022]</strong></span> The project
                        <a href="https://arxiv.org/abs/2203.03682">"Monocular Robot Navigation with Self-Supervised
                          Pretrained Vision Transformers"</a> was featured in
                        <a href="https://www.duckietown.org/archives/88017"> Duckietown's webpage!</a>
                      </li>
                      <li>
                        <span style="color:#ff0000"><strong>[April 2022]</strong></span> I have been accepted at
                        <a href="https://robotics-summerschool.ethz.ch/">ETH Robotics Summer School</a> 2022 as a
                        participant.
                      </li>
                      <li>
                        <span style="color:#ff0000"><strong>[March 2022]</strong></span> I was awarded by Le d√©partement
                        d'informatique et de recherche op√©rationnelle (DIRO) and Le minist√®re de l‚ÄôEnseignement
                        sup√©rieur du Qu√©bec with a scholarship.
                      </li>
                      <li>
                        <span style="color:#ff0000"><strong>[March 2022]</strong></span> The paper
                          <a href="https://arxiv.org/abs/2203.03682">"Monocular Robot Navigation with
                          Self-Supervised Pretrained Vision Transformers"</a> has been accepted for Poster presentation
                          at the 19th <a href="https://www.computerrobotvision.org/#"> Conference on Robotics and Vision
                          (CRV).</a>
                      </li>
                      <li>
                        <strong>[September 2021]</strong> I Started a research Master
                        in Computer Science at <a href="https://mila.quebec/en/">MILA</a> and <a
                          href="https://www.umontreal.ca/">Universit√© de Montr√©al</a> under the supervision of <a
                          href="https://liampaull.ca/">Liam Paull</a>. The focus will be in Artificial Intelligence and
                        Robotics.
                      </li>
                      <li>
                        <strong>[July 2021]</strong>The paper <a href="https://arxiv.org/abs/2108.06616">"Monocular
                          visual autonomous landing system for quadcopter drones using software in the loop"</a> was
                          accepted in the journal <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=62">
                          Aerospace & Electronics Systems Magazine</a> IEEE.
                      </li>
                      <li>
                        <strong>[July 2021]</strong> Successfully completed a Postgraduate Diploma in Artificial
                        Intelligence at <a href="https://www.uao.edu.co/">Universidad Autonoma de Occidente</a>.
                      </li>
                      <li>
                        <strong>[June 2021]</strong> I will join <a href="https://mila.quebec/en/">MILA</a> and <a
                          href="https://www.umontreal.ca/">Universit√© de Montr√©al</a>
                        as a MSc student under the supervision of <a href="https://liampaull.ca/">Liam Paull</a>
                        starting in Fall 2021.
                      </li>
                      <li>
                        <strong>[April 2021]</strong> The extended abstract <a
                          href="https://arxiv.org/abs/2105.11060">High-level camera-LiDAR fusion for 3D object detection
                          with
                          machine learning</a> was accepted to the <a
                          href="https://www.latinxinai.org/cvpr-2021-about">LatinX in AI Research Workshop</a> at <a
                          href="http://cvpr2021.thecvf.com/">CVPR</a>, 2021 as a poster presentation.
                      </li>
                      <li>
                        <strong>[December 2020]</strong> I am joining the <a href="https://whaleandjaguar.co/">Whale &
                          Jaguar</a>'s team as a Machine Learning Engineer.
                      </li>
                      <li>
                        <strong>[August 2020]</strong> I have been accepted into the <a
                          href="https://riiaa.org/en/home/">RIIAA</a> online summer school in RL and Bayesian inference
                        as an active track.
                      </li>
                      <li>
                        <strong>[August 2020]</strong> I Started a Postgraduate Diploma in Artificial Intelligence at <a
                          href="https://www.uao.edu.co/">Universidad Autonoma de Occidente</a>, the degree will end in
                        June 2021.
                      </li>
                      <li>
                        <strong>[Jul 2020]</strong> The paper "Monocular visual autonomous landing system for quadcopter
                        drones using software in the loop" has been sent to the journal <a
                          href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=62">Aerospace & Electronics
                          Systems Magazine</a> IEEE (Under review).
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <br>

          <!-- Publications -->
          <div class="containersmall" id="pub_sec">
            <table
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td colspan="2" style="padding:1%;width:100%">
                    <p style="text-align:center">
                      <heading>Publications</heading>
                    </p>
                  </td>
                  <td></td>
                </tr>

                <!-- DINO Real Project -->
                <tr onmouseout="dino_real_stop()" onmouseover="dino_real_start()">
                  <td
                      style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='dino_real_seg'>
                        <img src='images/dino_real_after.gif' width="160">
                      </div>
                      <img src='images/dino_real_before.jpg' width="160">
                    </div>
                    <script type="text/javascript">
                      function dino_real_start() {
                        document.getElementById('dino_real_seg').style.opacity = "1";
                      }

                      function dino_real_stop() {
                        document.getElementById('dino_real_seg').style.opacity = "0";
                      }
                      dino_real_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://sachamorin.github.io/dino/">
                      <papertitle>Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers
                      </papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>,
                    <a href="https://github.com/sachaMorin">Sacha Morin</a>,
                    <a href="https://liampaull.ca/">Liam Paull</a>
                    <br>
                    <em>Conference on Robotics and Vision (CRV) - (to appear)</em>, 2022 &nbsp <span
                          style="color:#ff0000"><strong>(Poster Presentation)</strong></span>
                    <br>
                    <div class="paper" id="dino_real">
                      <a href="https://github.com/sachaMorin/dino">code (model)</a> /
                      <a href="https://github.com/MikeS96/object-detection/tree/daffy">code (servoing)</a> /
                      <a href="https://arxiv.org/abs/2203.03682">arXiv</a> /
                      <a href="https://sachamorin.github.io/dino/">webpage</a> /
                      <a href="https://www.duckietown.org/archives/88017">duckietown coverage</a> /
                      <a href="data/duckieFormerPoster.png">poster</a> /
                      <a href="javascript:togglebib('dino_real')" class="togglebib">bibtex</a>
                      <pre id="myid" xml:space="preserve">
@article{saavedra2022monocular,
	title        = {Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers},
	author       = {Saavedra-Ruiz, Miguel and Morin, Sacha and Paull, Liam},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2203.03682}
}                     </pre>
                    </div>
                    <p></p>
                    <div class="paper" id="dino_real_description">
                      <h6><a href="javascript:togglebib('dino_real_description')" class="togglebib">Visual Servoing navigation using pre-trained Self-Supervised Vision Transformers.</a></h6>
                      <pre xml:space="preserve">
In this work, we consider the problem of learning a perception model for
monocular robot navigation using few annotated images. Using a Vision
Transformer (ViT) pretrained with a label-free self-supervised method, we
successfully train a coarse image segmentation model for the Duckietown
environment using 70 training images. Our model performs coarse image
segmentation at the 8x8  patch level, and the inference resolution can be
adjusted to balance prediction granularity and real-time perception constraints.
We study how best to adapt a ViT to our task and environment, and find that some
lightweight architectures can yield good single-image segmentations at a usable
frame rate, even on CPU. The resulting perception model is used as the backbone
for a simple yet robust visual servoing agent, which we deploy on a differential
drive mobile robot to perform two tasks: lane following and obstacle avoidance. </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Autonomous landing of a UAV Journal-->
                <tr onmouseout="dals_stop()" onmouseover="dals_start()">
                  <td
                          style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='dals_image'>
                        <img src='images/gazebo_dron_after.gif' width="160">
                      </div>
                      <img src='images/gazebo_dron_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function dals_start() {
                        document.getElementById('dals_image').style.opacity = "1";
                      }

                      function dals_stop() {
                        document.getElementById('dals_image').style.opacity = "0";
                      }
                      dals_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/autonomous_landing_uav">
                      <papertitle>
                        Monocular visual autonomous landing system for quadcopter drones using software in the loop
                      </papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>,
                    <a href="https://www.linkedin.com/in/ana-maria-pinto-b380b017a/">Ana Pinto</a>,
                    <a href="https://scholar.google.com.au/citations?user=x3M1JlAAAAAJ&hl=en">Victor Romero-Cano</a>
                    <br>
                    <em>IEEE Aerospace And Electronic Systems </em>, 2021 &nbsp <span
                      style="color:#ff0000"><strong>(Journal publication)</strong></span>
                    <br>

                    <div class="paper" id="dals">
                      <a href="https://github.com/MikeS96/autonomous_landing_uav">code</a> /
                      <a href="data/RedColsiPasto.pdf">poster</a> /
                      <a href="https://red.uao.edu.co/bitstream/10614/10754/5/T08388.pdf">thesis</a> /
                      <a href="https://arxiv.org/abs/2108.06616">arXiv</a> /
                      <a href="https://www.youtube.com/watch?v=2NWR9Otdz-s&feature=youtu.be">video</a> /

                      <a href="javascript:togglebib('dals')" class="togglebib">bibtex</a>
                      <pre xml:space="preserve">
@article{9656574,
	title        = {Monocular Visual Autonomous Landing System for Quadcopter Drones Using Software in the Loop},
	author       = {Saavedra-Ruiz, Miguel and Pinto-Vargas, Ana Maria and Romero-Cano, Victor},
	year         = 2022,
	journal      = {IEEE Aerospace and Electronic Systems Magazine},
	volume       = 37,
	number       = 5,
	pages        = {2--16},
	doi          = {10.1109/MAES.2021.3115208}
}  </pre>
                      <p></p>
                      <div class="paper" id="dals_description">
                        <h6><a href="javascript:togglebib('dals_description')" class="togglebib">Autonomous landing
                          system for a UAV on a terrestrial vehicle using robotics vision and control.</a>
                        </h6>
                        <pre xml:space="preserve">
My BEng degree project addressed the problem of the autonomous landing of a UAV
with a landing platform located on the top of a ground vehicle. The project
utilized vision-based techniques to detect the landing platform, a Kalman filter
was tailored for the tracking phase and finally, a PID controller sent control
commands to the flight controller of the UAV to land properly on the platform.
Rigorous assessments were conducted through the simulation of the whole robotic
stack with ROS and gazebo in the software in the loop provided by PX4.
Ultimately, the system was tested in a custom DJI F-450 and embedded in a Odroid
XU4. The system demonstrates a satisfactory performance and was able to land
with a mean error of ten centimeters from the center of the landing platform
(Implemented in Python, C++/Linux).

Autonomous landing is a capability that is essential to achieve the full
potential of multi-rotor drones in many social and industrial applications. The
implementation and testing of this capability on physical platforms is risky and
resource-intensive; hence, in order to ensure both a sound design process and a
safe deployment, simulations are required before implementing a physical
prototype. This paper presents the development of a monocular visual system,
using a software-in-the-loop methodology, that autonomously and efficiently
lands a quadcopter drone on a predefined landing pad, thus reducing the risks of
the physical testing stage. In addition to ensuring that the autonomous landing
system as a whole fulfils the design requirements using a Gazebo-based
simulation, our approach provides a tool for safe parameter tuning and design
testing prior to physical implementation. Finally, the proposed monocular
vision-only approach to landing pad tracking made it possible to effectively
implement the system in an F450 quadcopter drone with the standard computational
capabilities of an Odroid XU4 embedded processor.  </pre>
                      </div>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- 3D object detection CVPR @ LatinX-->
                <tr onmouseout="ml3d_stop()" onmouseover="ml3d_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='ml3d_image'>
                        <img src='images/3d_after.gif' width="160">
                      </div>
                      <img src='images/3d_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function ml3d_start() {
                        document.getElementById('ml3d_image').style.opacity = "1";
                      }

                      function ml3d_stop() {
                        document.getElementById('ml3d_image').style.opacity = "0";
                      }
                      ml3d_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/3d_obj_detection">
                      <papertitle>3D object detector for vehicles using classic Machine Learning</papertitle>
                    </a>
                    <br>
                    <a href="https://www.linkedin.com/in/gustavo-andres-salazar-gomez/">Gustavo Salazar</a>,
                    <strong>Miguel Saavedra-Ruiz</strong>,
                    <a href="https://scholar.google.com.au/citations?user=x3M1JlAAAAAJ&hl=en">Victor Romero-Cano</a>
                    <br>
                    <em>LatinX Workshop at CVPR</em>, 2021 &nbsp <span style="color:#ff0000"><strong>(Poster
                        Presentation)</strong></span>
                    <br>
                    <div class="paper" id="ml3d">
                      <a href="https://github.com/MikeS96/3d_obj_detection">code</a> /
                      <a href="https://arxiv.org/abs/2105.11060">arXiv</a> /
                      <a href="https://research.latinxinai.org/papers/cvpr/2021/png/21_poster_21.png">poster</a>
                      <p></p>

                      <div class="paper" id="ml3d_description">
                        <h6><a href="javascript:togglebib('ml3d_description')" class="togglebib">3D object detection of
                            vehicles in the NuScenes dataset using classic Machine learning such as DBSCAN and SVMs.</a>
                        </h6>
                        <pre xml:space="preserve">
3D object detection is a problem that has gained popularity among the research
community due to its extensiveset of application on autonomous navigation,
surveillance and pick-and-place. Most of the solutions proposed in the
state-of-the-art are based on deep learning techniques and present astonishing
results in terms of accuracy. Nevertheless, a set of problems inherits from this
sort of solutions such as the need of enormous tagged datasets, extensive
computational resources due to the complexity of the model and most of the time,
no real-time inference. This work proposes an end-to-end classic Machine
Learning (ML) pipeline to solve the 3D object detection problem for cars. The
proposed method is leveraged on the use of frustum region proposals to segment
and estimate the parameters of the amodal 3D bounding box. Here we do not deal
with the problem of 2D object detection as for most of the research community
this is considered solved with ConvolutionalNeural Networks (CNN).

This task is addressed employing different ML techniques such as RANSAC for road
segmentation and DBSCAN for clustering. Global features are extracted out of the
segmented point cloud using The Ensemble of Shape Functions (ESF). Some feature
are engineered through PCA and statistics. Finally, the amodal 3D bounding box
parameters are estimated through a SVR regressor. </pre>
                      </div>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Detection system CCRA-->
                <tr onmouseout="dtl_stop()" onmouseover="dtl_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='dtl_image'>
                        <img src='images/detection_ccra_after.png' width="160">
                      </div>
                      <img src='images/detection_ccra_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function dtl_start() {
                        document.getElementById('dtl_image').style.opacity = "1";
                      }

                      function dtl_stop() {
                        document.getElementById('dtl_image').style.opacity = "0";
                      }
                      dtl_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/autonomous_landing_uav/tree/master/object_detector">
                      <papertitle>Detection and tracking of a landing platform for aerial robotics applications
                      </papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>,
                    <a href="https://www.linkedin.com/in/ana-maria-pinto-b380b017a/">Ana Pinto</a>,
                    <a href="https://scholar.google.com.au/citations?user=x3M1JlAAAAAJ&hl=en">Victor Romero-Cano</a>
                    <br>
                    <em>CCRA</em>, 2018 &nbsp <span style="color:#ff0000"><strong>(Oral Presentation)</strong></span>
                    <br>
                    <div class="paper" id="dtl">
                      <a href="https://github.com/MikeS96/autonomous_landing_uav/tree/master/object_detector">code</a> /
                      <a href="https://www.youtube.com/watch?v=knH2Hju--3I&feature=emb_title">video</a> /
                      <a href="javascript:togglebib('dtl')" class="togglebib">bibtex</a>
                      <pre xml:space="preserve">
@inproceedings{8588112,
	title        = {Detection and tracking of a landing platform for aerial robotics applications},
	author       = {M. S. {Ruiz} and A. M. P. {Vargas} and V. R. {Cano}},
	year         = 2018,
	booktitle    = {2018 IEEE 2nd Colombian Conference on Robotics and Automation (CCRA)},
	volume       = {},
	number       = {},
	pages        = {1--6}
}                     </pre>
                      <p></p>
                      <div class="paper" id="dtl_description">
                        <h6><a href="javascript:togglebib('dtl_description')" class="togglebib">Object Detection and
                            tracking pipelines to detect a landing pad on the ground from a UAV.</a></h6>
                        <pre xml:space="preserve">
Aerial robotic applications need to be endowed with systems capable to
accurately locate objects of interest to perform specific tasks at hand. I
Developed an embedded vision-based landing platform detection and tracking
system with ROS and OpenCV. The system extended the capabilities of a SURF-based
feature detector-descriptor that makes detections of a landing pad alongside a
Kalman filter-based estimation module. The system demonstrated a considerable
improvement over only-detector methods, diminishing the detection error and
providing accurate estimations of the landing pad position (Implemented in
C++/Linux). </pre>
                      </div>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <tr onmouseout="ldp_stop()" onmouseover="ldp_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='ldp_image'>
                        <img src='images/localization_after.gif' width="160">
                      </div>
                      <img src='images/localization_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function ldp_start() {
                        document.getElementById('ldp_image').style.opacity = "1";
                      }

                      function ldp_stop() {
                        document.getElementById('ldp_image').style.opacity = "0";
                      }
                      ldp_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/autonomous_landing_uav/tree/master/object_detector">
                      <papertitle>Localization of a landing platform for a UAV</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>,
                    <a href="http://www.seanfanello.it/">Ana Pinto</a>,
                    <a href="https://research.google/people/105312/">Victor Romero-Cano</a>
                    <br>
                    <span style="color:#ff0000"><strong>(Poster Presentation)</strong></span>
                    <br>
                    <a href="https://github.com/MikeS96/autonomous_landing_uav/tree/master/object_detector">code</a> /
                    <a href="data/pendonUAO2018.pdf">poster</a>
                    <p></p>
                    <p>Localization of a landing pad located at the top of a ground vehicle with a UAV.</p>
                  </td>
                </tr>

              </tbody>
            </table>
          </div>
          <br>

          <!-- Projects -->
          <div class="containersmall" id="proj_sec">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <tr>
                  <td colspan="2" style="padding:2%;width:100%">
                    <p style="text-align:center">
                      <heading>Projects</heading>
                    </p>
                  </td>
                  <td></td>
                </tr>


                <!-- DINO Simulated Project -->
<!--                <tr onmouseout="dino_stop()" onmouseover="dino_start()">-->
<!--                  <td-->
<!--                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">-->
<!--                    <div class="one">-->
<!--                      <div class="two" id='dino_seg'>-->
<!--                        <img src='images/dino_after.gif' width="160">-->
<!--                      </div>-->
<!--                      <img src='images/dino_before.png' width="160">-->
<!--                    </div>-->
<!--                    <script type="text/javascript">-->
<!--                      function dino_start() {-->
<!--                        document.getElementById('dino_seg').style.opacity = "1";-->
<!--                      }-->

<!--                      function dino_stop() {-->
<!--                        document.getElementById('dino_seg').style.opacity = "0";-->
<!--                      }-->
<!--                      dino_stop()-->
<!--                    </script>-->
<!--                  </td>-->
<!--                  <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--                    <a href="https://github.com/sachaMorin/dino">-->
<!--                      <papertitle>Self-Supervised Learning : Applications to object detection and image segmentation</papertitle>-->
<!--                    </a>-->
<!--                    <br>-->
<!--                    <a href="https://github.com/sachaMorin">Sacha Morin</a>,-->
<!--                    <strong>Miguel Saavedra-Ruiz</strong>,-->
<!--                    Wei Fen-->
<!--                    <br>-->
<!--                    <a href="https://github.com/sachaMorin/dino">code</a>-->
<!--                    <p></p>-->
<!--                    <div class="paper" id="dino_description">-->
<!--                      <h6><a href="javascript:togglebib('dino_description')" class="togglebib">DINO features for object detection and sparse instance segmentation in duckie-town dataset.</a></h6>-->
<!--                      <pre xml:space="preserve">-->
<!--In this work, we assess the usefulness of embeddings from two Self-Supervised representation-->
<!--learning methods, DINO and MoBY, in two downstream computer vision applications: image-->
<!--segmentation and object detection. We use Duckietown image data for all of our experiments.-->
<!--Results demonstrate that indeed DINO Embeddings provide an excelling starting point for downstream applications.-->
<!--                </pre>-->
<!--                    </div>-->
<!--                  </td>-->
<!--                </tr>-->
<!--                &lt;!&ndash; Break of line&ndash;&gt;-->
<!--                <tr>-->
<!--                  <td colspan="2" style="padding:0; margin:0;">-->
<!--                    <hr style="padding:0; margin:0;">-->
<!--                  </td>-->
<!--                </tr>-->

                <!-- Transfer Learning -->
                <tr onmouseout="style_stop()" onmouseover="style_start()">
                  <td
                    style="padding-top:0%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='style_image'>
                        <img src='images/style_after.gif' width="160">
                      </div>
                      <img src='images/style_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function style_start() {
                        document.getElementById('style_image').style.opacity = "1";
                      }

                      function style_stop() {
                        document.getElementById('style_image').style.opacity = "0";
                      }
                      style_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/zestian56/style-transfer">
                      <papertitle>Style-transfer for the creation of aesthetic images</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>,
                    <a href="https://github.com/gsg213">Gustavo Salazar</a>,
                    <a href="https://github.com/zestian56">Sebastian Botero</a>
                    <br>
                    <a href="https://github.com/zestian56/style-transfer">code</a> /
                    <a href="data/styleTransfer.pdf">report (spanish)</a>
                    <p></p>
                    <div class="paper" id="style_description">
                      <h6><a href="javascript:togglebib('style_description')" class="togglebib">Style-transfer
                          implementarion based on the paper A neural algorithm of artistic style using VGG-19.</a></h6>
                      <pre xml:space="preserve">
Implementation of Style-Transfer based on the original paper 'A neural algorithm
of artistic style'. As stated by the authors, style-transfer is 'an artificial
system based on a Deep Neural Network that creates artistic images of high
perceptual quality'. In this work we aimed to replicate the original paper in a
Pytorch-based implementation using as backbone a VGG-19 model. The results
present how the implementation is capable to transfer the desired content and
style to a different image and even create an aesthetic result from scratch.

This work was locally deployed using docker-compose and web sockets in order the
create a seamlessly implementation of style-transfer with an intuitive GUI. </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Robotics Software -->
                <tr onmouseout="rsnd_stop()" onmouseover="rsnd_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='rsnd_image'>
                        <img src='images/RTAB_after.gif' width="160">
                      </div>
                      <img src='images/RTAB_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function rsnd_start() {
                        document.getElementById('rsnd_image').style.opacity = "1";
                      }

                      function rsnd_stop() {
                        document.getElementById('rsnd_image').style.opacity = "0";
                      }
                      rsnd_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/robotics_software_nd">
                      <papertitle>Robotics Software Engineer projects</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="https://github.com/MikeS96/robotics_software_nd">code</a>
                    <p></p>
                    <div class="paper" id="rsnd_description">
                      <h6><a href="javascript:togglebib('rsnd_description')" class="togglebib">Robot localization,
                          Mapping, SLAM, path planning and navigation.</a></h6>
                      <pre xml:space="preserve">
 Implementation of different robotics vision projects. These are based on the
robotics software engineer nanodegree provided by Udacity. All the projects were
developed with ROS and tested in a gazebo-based simulated environment. Different
topics were addressed such as Gazebo basic, ROS, robot localization, Mapping,
SLAM, navigation and path planning. The projects are listed below

    * How to use the model and building editor, plugins in Gazebo and more.
    * Creating a ball chaser robot in ROS and Gazebo.
    * Localization through Kalman Filter, Monte Carlo methods and ACML.
    * Occupancy grid map generation, GridBased Fast Slam ROS package and
      RTAB-Map SLAM.
    * The A* algorithm and robot navigation.</pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- VO and VIO -->
                <tr onmouseout="vio_stop()" onmouseover="vio_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='vio_image'>
                        <img src='images/vio_after.gif' width="160">
                      </div>
                      <img src='images/vio_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function vio_start() {
                        document.getElementById('vio_image').style.opacity = "1";
                      }

                      function vio_stop() {
                        document.getElementById('vio_image').style.opacity = "0";
                      }
                      vio_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/vio_quadrotor">
                      <papertitle>Stereo Visual Odometry (VO) and Visual Inertial Odometry (VIO) with EFK in a
                        quad-rotor</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="https://github.com/MikeS96/vio_quadrotor">code</a>
                    <p></p>

                    <div class="paper" id="vio_description">
                      <h6><a href="javascript:togglebib('vio_description')" class="togglebib">Stereo Visual Odometry and
                          Visual Inertial Odometry pipelines to estimate the pose of a quad-rotor.</a></h6>
                      <pre xml:space="preserve">
Implemented Stereo Visual Odometry and Visual Inertial Odometry pipelines to
estimate the pose of a quad-rotor. The system works by taking subsequent image
pairs and matching features throughout the test. Once those features are
obtained, 3d-points coordinates were retrieved with the depth map of the images
and the extrinsic camera calibration  matrix. Finally, the trajectory is
estimated using 3D-2D Perspective-n-Point (PNP). As an additional step, the VO
trajectory was used with the IMU data in an Error-State Extended Kalman Filter
to estimate the pose even when most of the VO observations were dropped. Both VO
and VIO showed good results estimating the trajectory of the UAV (Implemented in
Python/Linux). </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- SITL -->
                <tr onmouseout="sitl_stop()" onmouseover="sitl_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='sitl_image'>
                        <img src='images/simu_landing_after.gif' width="160">
                      </div>
                      <img src='images/simu_landing_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function sitl_start() {
                        document.getElementById('sitl_image').style.opacity = "1";
                      }

                      function sitl_stop() {
                        document.getElementById('sitl_image').style.opacity = "0";
                      }
                      sitl_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/autonomous_landing_uav">
                      <papertitle>Simulation of a landing system for a UAV in Gazebo</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="https://github.com/MikeS96/autonomous_landing_uav">code</a> /
                    <a href="https://www.youtube.com/watch?v=41xJj8g8Ce8&feature=youtu.be">video</a>
                    <p></p>
                    <div class="paper" id="sitl_description">
                      <h6><a href="javascript:togglebib('sitl_description')" class="togglebib">Simulation of an
                          autonomous landing system for a UAV with Gazebo, ROS and the Software in the loop provided by
                          PX4.</a></h6>
                      <pre xml:space="preserve">
Simulated an autonomous landing system for a UAV with Gazebo, ROS and the
Software in the loop provided by PX4. The project consisted in the development
of different packages in C++ and Python which allowed the assessment of an
autonomous landing system. This robotics simulation allowed the thoroughly
development and evaluation of a landing pipeline for a UAV (Implemented in
Python, C++/ Linux). </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- RL Projects -->
                <tr onmouseout="rlSpe_stop()" onmouseover="rlSpe_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='rlSpe_image'>
                        <img src='images/lunar_lander_after.gif' width="160">
                      </div>
                      <img src='images/lunar_lander_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function rlSpe_start() {
                        document.getElementById('rlSpe_image').style.opacity = "1";
                      }

                      function rlSpe_stop() {
                        document.getElementById('rlSpe_image').style.opacity = "0";
                      }
                      rlSpe_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/rl_openai">
                      <papertitle>‚ÄãReinforcement Learning Specialization Projects</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="https://github.com/MikeS96/rl_openai">code</a> /
                    <a href="https://www.youtube.com/watch?v=rJE_hfkvKvk&feature=youtu.be">video</a>
                    <p></p>
                    <div class="paper" id="rlSpe_description">
                      <h6><a href="javascript:togglebib('rlSpe_description')" class="togglebib">Lunar Lander, Mountain
                          Car and Pendulum classical control tasks solved using RL.</a></h6>
                      <pre xml:space="preserve">
Trained a lunar lander in a simulated environment with Reinforcement Learning.
The agent was implemented with the Expected-Sarsa algorithm and used a Neural
networkfor for action-values approximation. The algorithm was capable to do
planning steps with experience replay and learn a policy for the landing of the
agent. Thorughout the specialization I implemented different projects, some of
those are listed below.

    * Solved a Gridworld city with Dynamic programming to find an optimal
      policie.
    * Implemented a Dyna-Q and Dyna-Q+ algorithms in a changing maze environment
      to assess the performance of planning methods in RL.
    * Implemented an Average Reward Softmax Actor-Critic algorithm using
      Tile-coding to solve the Pendulum Swing-Up continuous problem.
    * Solved the Mountain car and Lunar Lander problems. </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Teleop -->
                <tr onmouseout="teleop_stop()" onmouseover="teleop_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='teleop_image'>
                        <img src='images/teleop_after.gif' width="160">
                      </div>
                      <img src='images/teleop_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function teleop_start() {
                        document.getElementById('teleop_image').style.opacity = "1";
                      }

                      function teleop_stop() {
                        document.getElementById('teleop_image').style.opacity = "0";
                      }
                      teleop_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://github.com/MikeS96/autonomous_landing_uav/tree/master/key_teleop">
                      <papertitle>Teleoperation system for a car-like robot</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="https://github.com/MikeS96/autonomous_landing_uav/tree/master/key_teleop">code</a> /
                    <a href="https://www.youtube.com/watch?v=54QGT7Ud98s&feature=youtu.be">video</a>
                    <p></p>

                    <div class="paper" id="teleop_description">
                      <h6><a href="javascript:togglebib('teleop_description')" class="togglebib">Teleoperation system
                          for a car-like robot through mathematical modelling.</a></h6>
                      <pre xml:space="preserve">
Mathematical modelling of unmanned groundvehicles (UGV) is a well studied
problem in robotics and essential for the control of a robot. Developed a
teleoperation system for a car-like robot. The system received velocities in the
local coordinate frame of the robot and through the inverse kinematics model of
the vehicle these velocity commands were transformed to wheels‚Äô speed and sent
to the vehicle‚Äôs motors. This project was embedded in a Raspberry Pi3 to allow
the remote control of the vehicle with a host computer through WIFI (Implemented
in C/Linux). </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Turtlebot -->
                <tr onmouseout="mturtle_stop()" onmouseover="mturtle_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='mturtle_image'>
                        <img src='images/map_turtle_after.png' width="160">
                      </div>
                      <img src='images/map_turtle_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function mturtle_start() {
                        document.getElementById('mturtle_image').style.opacity = "1";
                      }

                      function mturtle_stop() {
                        document.getElementById('mturtle_image').style.opacity = "0";
                      }
                      mturtle_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="data/slidesturtle.pdf">
                      <papertitle>Mapping and localization in indoors with Turtlebot 2</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="data/slidesturtle.pdf">slides</a> /
                    <a href="data/finalreport.pdf">report</a>
                    <p></p>
                    <div class="paper" id="mturtle_description">
                      <h6><a href="javascript:togglebib('mturtle_description')" class="togglebib">Simulation of a
                          localization and mapping system (laser-based SLAM) for a turtlebot2 in indoors.</a></h6>
                      <pre xml:space="preserve">
Equipping robotic systems with novel localization and navigation stacks is
crucial for autonomous navigation. A localization and mapping system
(laser-based SLAM) for a turtlebot2 in indoors was simulated. The system was
capable to accurately localize the robot in a previously mapped environment and
subsequently navigate to a specific position in an occupancy grid map
(Implemented in C++/Linux). </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Self-Driving Cars Specialization -->
                <tr onmouseout="carsSpe_stop()" onmouseover="carsSpe_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='carsSpe_image'>
                        <img src='images/cars_after.gif' width="160">
                      </div>
                      <img src='images/cars_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function carsSpe_start() {
                        document.getElementById('carsSpe_image').style.opacity = "1";
                      }

                      function carsSpe_stop() {
                        document.getElementById('carsSpe_image').style.opacity = "0";
                      }
                      carsSpe_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://www.youtube.com/watch?v=f-k9wR0Ty8o">
                      <papertitle>Self-Driving Cars Specialization Projects</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <a href="https://www.youtube.com/watch?v=f-k9wR0Ty8o">video1</a> /
                    <a href="https://www.youtube.com/watch?v=1I6UJQyJovs">video2</a>
                    <p></p>

                    <div class="paper" id="carsSpe_description">
                      <h6><a href="javascript:togglebib('carsSpe_description')" class="togglebib">Wheeled-robot
                          mathematical modelling through dynamical modelling (tire model), lateral and longitudinal
                          control, state estimation with Kalman filters, visual perception and motion planning for
                          self-driving vehicles.</a></h6>
                      <pre xml:space="preserve">
This work involved four capstone projects in the area of self-driving cars.
Topics such as wheeled-robot mathematical modelling through dynamical modelling
(tire model), lateral and longitudinal control, state estimation with Kalman
filters, visual perception and motion planning were addressed. Most of the
projects were tested in the Carla Simulator to assess performance. A description
of the projects developed are presented below.

    * Control of a car-like robot through a longitudinal and lateral controller.
      The longitudinal controller was implemented with a PID and the lateral
      controller was a cross-track error controller.
    * Implementation of an error state extended Kalman Filter for the estimation
      of the trajectory of a vehicle. The filter fused information from a GNSS
      and IMU alongside the dynamic model of the vehicle to produce an accurate
      destimation of its trajectory on the space.
    * Robotics perception stack which detected the drivable space of the vehicle
      through image segmentation. Canny edge detector was used to detect the
      lines of the road and a depth representation of the scene was employed to
      estimate the distance-to-objects in the road and avoid collision using only
      image-based methods.
    * Implemented a navigation stack in the Carla simulator with the use of grid
      world representations  and state machines for a simple navigation strategy. </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- License Plate Project -->
                <tr onmouseout="lplate_stop()" onmouseover="lplate_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='lplate_image'>
                        <img src='images/license_plate_after.png' width="160">
                      </div>
                      <img src='images/license_plate_before.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function lplate_start() {
                        document.getElementById('lplate_image').style.opacity = "1";
                      }

                      function lplate_stop() {
                        document.getElementById('lplate_image').style.opacity = "0";
                      }
                      lplate_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="data/licensePlate.txt">
                      <papertitle>Low-cost license plate recognition system based on CNN</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <p></p>

                    <div class="paper" id="lplate_description">
                      <h6><a href="javascript:togglebib('lplate_description')" class="togglebib">Implemented a low-cost
                          license plate recognition systems using deep learning techniques.</a></h6>
                      <pre xml:space="preserve">
Automatic license plate recognition (LPR) is indispensable for the admission and
flow control of vehicles into parking lots orcondominiums.  Generally, these
systems are based on classic computer vision techniques owing to their
processing speed, however, these approaches can lead to inaccurate detections
and vague performance on non-ideal environmental conditions. My work tried to
surpass these setbacks with the implementation of an image-based plate
recognition system using convolutional neural networks (CNN) to enhance the
current methods. The system was optimized and embedded in a Nvidia Jetson Nano
to run in a low-cost computer at a recognition rate of 100ms per plate making it
ideal to operate in the places mentioned before (Implemented in Python, C++,
TensorRT/Linux). </pre>
                    </div>
                  </td>
                </tr>
                <!-- Break of line-->
                <tr>
                  <td colspan="2" style="padding:0; margin:0;">
                    <hr style="padding:0; margin:0;">
                  </td>
                </tr>

                <!-- Flow-Control with heatmaps -->
                <tr onmouseout="flowc_stop()" onmouseover="flowc_start()">
                  <td
                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='flowc_image'>
                        <img src='images/flow_control_after.png' width="160">
                      </div>
                      <img src='images/flow_control_before.jpg' width="160">
                    </div>
                    <script type="text/javascript">
                      function flowc_start() {
                        document.getElementById('flowc_image').style.opacity = "1";
                      }

                      function flowc_stop() {
                        document.getElementById('flowc_image').style.opacity = "0";
                      }
                      flowc_stop()
                    </script>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="data/flowControl.txt">
                      <papertitle>Flow control with heatmaps in indoors</papertitle>
                    </a>
                    <br>
                    <strong>Miguel Saavedra-Ruiz</strong>
                    <br>
                    <p></p>

                    <div class="paper" id="flowc_description">
                      <h6><a href="javascript:togglebib('flowc_description')" class="togglebib">Heat map generator based
                          on computer vision techniques to stochastically estimate the most visited areas in an indoor
                          space with a monocular camera.</a></h6>
                      <pre xml:space="preserve">
Implemented a heat map generator based on computer vision techniques to
stochastically estimate the most visited areas in an indoor space with a
monocular camera. A feature tracker method was used to estimate the average flow
of persons and a deep convolutional neural network was employed to obtain the
segmentation of the floor in the scene. This information was merge together to
gather relevant information about the people habits in shopping centers or
crowded areas (Implemented in Python/ Linux) </pre>
                    </div>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <br>

<!--                &lt;!&ndash; Calendula Flower Experiments &ndash;&gt;-->
<!--                <tr onmouseout="calenF_stop()" onmouseover="calenF_start()">-->
<!--                  <td-->
<!--                    style="padding-top:2%;padding-bottom:2%;padding-left:7.5%;padding-right:0%;width:25%;vertical-align:middle">-->
<!--                    <div class="one">-->
<!--                      <div class="two" id='calenF_image'>-->
<!--                        <img src='images/calendula_after.jpg' width="160">-->
<!--                      </div>-->
<!--                      <img src='images/calendula_after.jpg' width="160">-->
<!--                    </div>-->
<!--                    <script type="text/javascript">-->
<!--                      function calenF_start() {-->
<!--                        document.getElementById('calenF_image').style.opacity = "1";-->
<!--                      }-->

<!--                      function calenF_stop() {-->
<!--                        document.getElementById('calenF_image').style.opacity = "0";-->
<!--                      }-->
<!--                      calenF_stop()-->
<!--                    </script>-->
<!--                  </td>-->
<!--                  <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--                    <a href="data/calendulaFlower.txt">-->
<!--                      <papertitle>‚ÄãCalendula Flower Classification System</papertitle>-->
<!--                    </a>-->
<!--                    <br>-->
<!--                    <strong>Miguel Saavedra-Ruiz</strong>-->
<!--                    <br>-->
<!--                    <p></p>-->

<!--                    <div class="paper" id="calenF_description">-->
<!--                      <h6><a href="javascript:togglebib('calenF_description')" class="togglebib">Implementation of a-->
<!--                          low-cost Calendula flower classification system using CNN.</a></h6>-->
<!--                      <pre xml:space="preserve">-->
<!--Implemented a low cost Calendula flower classification system with Tensorflow, Android Studio and-->
<!--Arduino. The system used a deep convolutional neural network inception v3 trained with different-->
<!--Calendula flowers. When the system detected a Calendula flower, it sent a signal to an Arduino-->
<!--board and moved a servomotor to a specific point which allowed the correct classification of the-->
<!--flowers through a transportation band (Implemented in C, Python/Linux). </pre>-->
<!--                    </div>-->
<!--                  </td>-->
<!--                </tr>-->



          <!-- Certificates -->
          <div class="containersmall" id="others">
            <table
              style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td
                    style="padding-top:1%;padding-bottom:1%;padding-left:4%;padding-right:4%;width:100%;vertical-align:middle">
                    <p style="text-align:center" id="cert_sec">
                      <heading>Courses and Certifications</heading>
                    </p>
                    <ul>
                      <li>
                        <strong>Reinforcement Learning</strong> by University of Alberta & Alberta Machine Intelligence
                        Institute on Coursera. Certificate earned at June 21, 2020. <a
                          href="https://www.coursera.org/account/accomplishments/specialization/certificate/ZUNADAFYKHPH">[Credential]</a>
                      </li>
                      <li>
                        <strong>Self-Driving Cars</strong> a 4-course specialization by University of Toronto on
                        Coursera. Specialization Certificate earned on June 5, 2019. <a
                          href="https://www.coursera.org/account/accomplishments/specialization/certificate/78KLAA9EGVRP">[Credential]</a>
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>

          <!-- Acknowledgements -->
          <table
            style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding-top:1%;padding-bottom:0%;padding-left:0%;padding-right:0%">
                  <p style="color:#333">
                    <strong>Updated</strong> May 22th 2022
                  </p>
                </td>
                <td style="padding-top:1%;padding-bottom:0%;padding-left:0%;padding-right:0%;text-align:right">
                  <p style="color:#333">
                    This template was shamelessly stolen from <a href="https://jonbarron.info/">here</a> and <a
                      href="https://gkioxari.github.io/">here</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
  <script xml:space="preserve" language="JavaScript">
    hideallbibs();
  </script>
</body>

</html>
